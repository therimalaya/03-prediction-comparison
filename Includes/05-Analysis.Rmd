# Statistical Analysis 

```{r knitr_setup}
options(knitr.kable.NA = '')
docs_format <- ifelse(knitr::is_latex_output(), "latex", "html")
print_manova <- function(tbl, format = docs_format, level = 0.05, ...){
  signif_idx <- which(tbl[, ncol(tbl)] <= level)
  knitr::kable(
    x = tbl, digits = 3, booktabs = TRUE,
    longtable = TRUE, format = format,
    escape = FALSE, linesep = "", ...) %>%
    kableExtra::kable_styling(latex_options = c("repeat_header")) %>%
    kableExtra::column_spec(1:7, monospace = TRUE) %>%
    kableExtra::row_spec(signif_idx, color = "red")
}
```

In order to carry out a proper statistical comparison, a multivariate analysis of variance (MANOVA) is used with minimum prediction error corresponding to each response variables for all design and their replicates. The third order interaction of simulation parameters (`p`, `gamma`, `eta` and `relpos`) and `Methods` is used as independent factors as \@ref(eq:expanded-model).

\begin{equation}
\mathbf{y}_{abcdef} = \boldsymbol{\mu} + (\texttt{p}_a + \texttt{gamma}_b + \texttt{eta}_c + \texttt{relpos}_d + \texttt{Methods}_e)^3 + \boldsymbol{\varepsilon}_{abcdef}
(\#eq:expanded-model)
\end{equation}

where, $\mathbf{y}_{abcdef}$ is a vector of prediction error for factors,

- $\texttt{p}_a =$ `r catvec(opts$p)`
- $\texttt{gamma}_b=$ `r catvec(opts$gamma)`
- $\texttt{eta}_c=$ `r catvec(opts$eta)`
- $\texttt{relpos}_d=$ `r catvec(opts$relpos)`
- $\texttt{Methods}_e=$ `r catvec(map_chr(mthds, ~paste0("\\texttt{", .x, "}")))`

In concise vector form, we can write as \@ref(eq:full-model).

\begin{equation}
\mathbf{y} = \boldsymbol{\mu} + (\texttt{p} + \texttt{gamma} + \texttt{eta} + \texttt{relpos} + \texttt{Methods})^3 + \boldsymbol{\varepsilon}
(\#eq:full-model)
\end{equation}
where, $\mathbf{y}$ is the vector of prediction error corresponding to response $y_j, j = 1, \ldots 4$.

Prediction methods also varies on number of components they use to get the minimum prediction error. A similar model as \@ref(eq:expanded-model) is used with $\mathbf{y}_{abcdef}$ as the number of components used to get the minimum prediction error. Here Pellai's trace is used for evaluating these model.

```{r prediction-model}
## Full Prediction Model
pred_mdl <- lm(
  formula = cbind(Y1, Y2, Y3, Y4) ~ (p + gamma + eta + relpos + Method) ^ 3,
  data = pred_dta_spread)

## Full Component Model
comp_mdl <- lm(
  formula = cbind(Y1, Y2, Y3, Y4) ~ (p + gamma + eta + relpos + Method) ^ 3,
  data = comp_dta_spread)

## Anova for Full Prediction Model
pred_aov <- anova(pred_mdl) %>%
  as.data.frame() %>%
  rownames_to_column('Factors') %>%
  as_tibble()

## Anova for Full Component Model
comp_aov <- anova(comp_mdl) %>%
  as.data.frame() %>%
  rownames_to_column('Factors') %>%
  as_tibble()

## Joining both tables
aov_df <- bind_rows(list(Pred = pred_aov, Comp = comp_aov), .id = "Type")
```


```{r prediction-model-plot, fig.width=9, out.width='100%', fig.asp=0.5, fig.cap="Pillai Statistic and F-value for the MANOVA model. The bar represents the F-value and the text labels are Pillai Statistic for corresponding factor."}
model_labels <- c(
  Comp = "Model: Number of Components",
  Pred = "Model: Prediction Error"
)
aov_df %>%
  filter(!(Factors %in% c('Residuals', '(Intercept)'))) %>%
  select(Model = Type, Factors, Pillai, Fvalue = `approx F`, Pvalue = `Pr(>F)`) %>%
  mutate(Pvalue = ifelse(Pvalue < 0.05, "<0.05", ">=0.05")) %>% 
  ggplot(aes(reorder(Factors, log1p(Fvalue)), log1p(Fvalue), fill = Pvalue)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Pillai, 2)), family = 'mono', angle = 90, hjust = "inward") +
  facet_grid(cols = vars(Model), scales = 'free_y', 
             labeller = labeller(Model = model_labels)) +
  theme_grey(base_family = "mono") +
  theme(legend.position = c(0.2, 0.9),
        legend.direction = 'horizontal',
        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(x = NULL, y = "log1p(Value)") +
  scale_y_continuous(trans = "log1p", breaks = scales::pretty_breaks(n=6))
```

SOME OBSERVATIONS:

- All main effects except `p` are significant and has large effect on both minimum number of components and prediction error.
- Position of relevant components have largest effect on prediction error. In case of minimum number of components, multicollineary also have largest effect in addition to the position of relevant components.
- However based on pillai trace statistic, Method has the lastest effect on both of the model.
- The interactions `p:gamma` and `eta:relpos:Method` is significant in prediction error model but not in minim number of components model. However, all of these interactions have small pillai statistic.
- In case of Number of components model interaction effects `gamma:eta:Method`, `p:Method` and `gamma:eta:relpos` are significant but not in the case of prediciton error model. Similar to previous point, they too have small pillai statistic.

## Effect Analysis

ON PREDICTION ERROR MODEL:

- It would be desirable to observe effect of these interactions. Figure \@ref(fig:pred-eff-plots) (left) shows clear difference of `eta` for a given `relpos`. The plot also shows a clear difference in effect of methods on prediction error.
- Figure \@ref(fig:pred-eff-plots) (right) shows effect of `gamma` for a given `method` and `eta`. It shows that these methods gives low prediction error is high multicollinear situations. 

```{r pred-eff-plots, fig.width=7, out.width='100%', fig.cap='Effect plot of some interactions of the multivariate linear model', fig.asp = 0.6}
thm <- theme(plot.title = element_blank(),
             plot.subtitle = element_blank(),
             legend.position = "bottom",
             axis.title = element_blank())
plt1 <- eff_df("eta:relpos:Method", pred_mdl) %>% 
  eff_plot3(reorder = TRUE, labeller = label_both) +
  theme_grey(base_family = "mono") +
  thm
plt2 <- eff_df("gamma:eta:Method", pred_mdl) %>% 
  eff_plot3(reorder = TRUE, labeller = label_both) +
  theme_grey(base_family = "mono") +
  thm
plt <- gridExtra::arrangeGrob(plt1, plt2, ncol = 2, 
                       bottom="Method", padding = unit(0.04, 'npc'),
                       left = "Fitted Prediction Error")
grid::grid.newpage()
grid::grid.draw(plt)
```

ON MINIMUM NUMBER OF COMPONENTS MODEL:

- Figure \@ref(fig:comp-eff-plots) (left) shows that `xenv` has used minimum number of components followed by `senv` methods than others in order to get their minimum prediction error.
- The same figure also suggest that the minimum number of components used by `PLS1`, `PLS2` and `PCR` vary and has high effect of `eta`. The `senv` model which consider both X and Y correlation structure while estimating regression coefficients has smallest variation of number of components used for different `eta` parameters.
- Figure \@ref(fig:comp-eff-plots) (right) shows that in case of low multicollinearity in the model `PLS` methods are used less number of components than `PCR`. This is expected since, `PLS` methods consider the covariance structure of predictor and response which `PCR` does not.


```{r comp-eff-plots, fig.width=7, out.width='100%', fig.cap='Effect plot of some interactions of the multivariate linear model', fig.asp = 0.6}
thm <- theme(plot.title = element_blank(),
             plot.subtitle = element_blank(),
             legend.position = "bottom",
             axis.title = element_blank())
plt1 <- eff_df("eta:relpos:Method", comp_mdl) %>% 
  eff_plot3(reorder = TRUE, labeller = label_both) +
  theme_grey(base_family = "mono") +
  thm
plt2 <- eff_df("gamma:eta:Method", comp_mdl) %>% 
  eff_plot3(reorder = TRUE, labeller = label_both) +
  theme_grey(base_family = "mono") +
  thm
plt <- gridExtra::arrangeGrob(plt1, plt2, ncol = 2, 
                       bottom="Method", padding = unit(0.04, 'npc'),
                       left = "Fitted Prediction Error")
grid::grid.newpage()
grid::grid.draw(plt)
```


ADDITIONAL OBSERVATIONS:

- Place for variable selection
- A PLS model is used to cross-validate the effects of these factors. The loading plot for the same model but only with second order interaction is in figure \@ref(fig:pls-loadings).

## A partial least square analysis on the model

<!-- Analysis using PLS model -->

```{r pls-model}
pred_pls_mdl <- plsr(
  formula = cbind(Y1, Y2, Y3, Y4) ~ (p + gamma + eta + relpos + Method) ^ 2, 
  data = pred_dta_spread, validation = "CV", segments = 10, scale = TRUE)
comp_pls_mdl <- plsr(
  formula = cbind(Y1, Y2, Y3, Y4) ~ (p + gamma + eta + relpos + Method) ^ 2, 
  data = comp_dta_spread, validation = "CV", segments = 10, scale = TRUE)
```

```{r, fig.asp=1, fig.width=8}
ncomp <- c(1)
pred_pls_loadings <- as.data.frame(loadings(pred_pls_mdl)[]) %>% 
    rownames_to_column("Factor") %>% 
    as_tibble() %>% 
    select_at(c(1, ncomp + 1))
comp_pls_loadings <- as.data.frame(loadings(comp_pls_mdl)[]) %>% 
    rownames_to_column("Factor") %>% 
    as_tibble() %>% 
    select_at(c(1, ncomp + 1))
pls_loadings <- bind_rows(
  list(Pred = pred_pls_loadings, 
       Comp = comp_pls_loadings), 
  .id = "Type")
```

```{r pls-loadings, warning=FALSE, message=FALSE, fig.width=9, out.width='100%', fig.cap='PLS Loadings (Component 1)', fig.asp = 0.6}
pls_loadings %>% 
  gather(Components, Loadings, -c(1:2)) %>% 
  mutate(Factor_ = str_remove_all(Factor, "[0-9, .]+")) %>% 
  mutate(Factor_ = str_remove_all(Factor_, "PLS|PCR|Xenv|Senv")) %>% 
  ggplot(aes(Factor, Loadings, color = Type)) +
  geom_hline(yintercept = 0, color = "darkgrey", linetype = 2) +
  geom_line(aes(group = Type)) +
  geom_point(shape = 21, size = 0.5) +
  facet_grid(Components ~ Factor_, labeller = labeller(
    # Type = gsub(": ", ":\n", model_labels),
    Factor_ = function(x) gsub(":", "\n", x),
    .multi_line = TRUE), scales = 'free_x', 
    space = 'free_x', drop = TRUE) +
  theme_grey(base_family = "mono") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
        strip.text.x = element_text(angle = 90),
        legend.position = "top") +
  scale_color_brewer(palette = "Set1") +
  labs(x = NULL)

```


SOME OBSERVATIONS:
```{r}
pred_expl_var <- explvar(pred_pls_mdl)
comp_expl_var <- explvar(comp_pls_mdl)
```

- The loadings for first components plotted in Figure-\@ref(fig:pls-loadings) has clearly separated the envelope models from the rest by giving positive loading for them and negative for the rest.
- This components has only explained `r pred_expl_var[1]` in prediction error model and `r comp_expl_var[1]` in minimum components model.


CONFUSION:

- The explained variation by each of these components is not in decending order as each successive components of PLS model is supposed to explain the maximum covarinace between predictor and response.