# Exploration

The structure of final data for further analysis contains five factors, prediction methods, number of components, replications and prediction error for `r num_vec[unique(opts$m)]` responses. The prediction error is computed for `r num_vec[unique(opts$m)]` responses using 0 to 10 predictor components for each 50 replicates using \@ref(eq:pred-error). Thus there are `r nrow(design)` (design) $\times$ `r length(mthds)` (methods) $\times$ 11 (number of components) $\times$ 50 (replications), i.e. `r nrow(design) * length(mthds) * 11 * 50` observations. Here the variables `Y1` to `Y4` corresponds to prediction error of respective response variables.


```{r data-together}
pred_dta <- design_chr %>%
    select_if(function(x) n_distinct(x) > 1) %>%
    mutate(Design = as.character(1:n())) %>%
    mutate_at(vars(p, gamma, relpos, eta), as.factor) %>%
    right_join(
        pred_error %>%
        filter(!Method %in% c("Ridge", "Lasso")),
        by = "Design") %>%
    mutate_if(is.character, as.factor) %>%
    mutate_at("p", as.factor) %>%
    mutate(Response = paste0("Y", Response))
pred_min <- pred_dta %>%
    group_by(p, relpos, eta, gamma, Method, Replication, Response) %>%
    summarize(Pred_Error = min(Pred_Error)) %>%
    spread(Response, Pred_Error)
pred_spread_df <- pred_dta %>%
    as.data.frame() %>%
    select(-Design, -q) %>%
    spread(Response, Pred_Error)
comp_min <- pred_dta %>%
    filter(!Method %in% c("Ridge", "Lasso")) %>%
    group_by(p, relpos, eta, gamma, Method, Replication, Response) %>%
    summarize(ncomp = Tuning_Param[which.min(Pred_Error)]) %>%
    spread(Response, ncomp)
```


Here we will not only focus on the minimum prediction error that a method can obtain but also the number of components they use to get that error. So, the dataset discussed above is summarized to construct following two smaller datasets. Let us call them _Error Dataset_ and _Component Dataset_.

- _Error Dataset_: One with minimum prediction error that a method can give using arbitrary number of components
- _Component Dataset_: Another with the number of components that the method has used to give that minimum prediction error

The [Statistical Analysis] section will perform multivariate analysis using MANOVA on these datasets however, this section focus more on exploring the variation in the prediction error and number of components due to our design parameters and prediction methods through plots.

Here we have a) four vectors of minimum prediction error and b) four vectors of corresponding number of components. The following exploration is based on the scores of principal components of these two matrices. The analysis will progress as in the Figure \@ref(fig:data-transform-plot) in the case of both datasets.

(ref:data-transform) Analysis of density of principal components of prediction erorr ror each response.

```{r data-transform-plot, fig.cap="(ref:data-transform)", fig.asp=0.5, out.width='90%', fig.align='center'}
plot_data_transform()
```


```{r pca}
pred_pca <- with(pred_min, prcomp(cbind(Y1, Y2, Y3, Y4)))
expl_var <- explvar(pred_pca) %>% round(2)
pred_dta_with_pc <- bind_cols(pred_min, as.data.frame(scores(pred_pca)[]))

comp_pca <- with(comp_min, prcomp(cbind(Y1, Y2, Y3, Y4)))
comp_expl_var <- explvar(comp_pca) %>% round(2)
comp_dta_with_pc <- bind_cols(comp_min, as.data.frame(scores(comp_pca)[]))
```

```{r pca-scatter, fig.cap="Exploration of Principal Components of Prediction Errors.", out.width="100%", fig.width=7, dpi=150, fig.asp=0.5, eval=FALSE}
ggplot(pred_dta_with_pc, aes(PC1, PC2, color = relpos)) +
  geom_hline(yintercept = 0, color = "darkgray", linetype = 2) +
  geom_vline(xintercept = 0, color = "darkgray", linetype = 2) +
  geom_point(alpha = 0.7, size = 1, shape = 21) +
  facet_grid(gamma ~ Method,
             labeller = labeller(eta = label_both, gamma = label_both)) +
  theme_grey(base_family = 'mono') +
  theme(legend.position = "bottom",
        strip.text = element_text(family = "mono")) +
  labs(x = paste0("PC1(", expl_var[1], "%)"),
       y = paste0("PC2(", expl_var[2], "%)")) +
  guides(colour = guide_legend(override.aes = list(size = 3))) +
  ggtitle("Principal Components Analysis of prediction errors") +
  scale_x_continuous(breaks = scales::pretty_breaks(3)) +
  scale_color_brewer(palette = "Set1")
```


(ref:pred-hist) Density of first principal component of prediction error matrix subdivided by methods, gamma and eta and grouped by relpos.

```{r pred-pca-hist-mthd-gamma-relpos, message=FALSE, warning=FALSE, fig.cap="(ref:pred-hist)", fig.pos="!htb"}
pc_density_plot <- function(dta, expl_var) {
  dta %>% 
  ggplot(aes(PC1, eta, fill = relpos)) +
  geom_density_ridges(
      scale = 0.9,
      alpha = 0.4, size = 0.25) +
  geom_density_ridges(
    scale = 0.95,
    alpha = 0.2, size = 0.25,
    stat = "binline", bins = 30) +
  facet_grid(
    gamma ~ Method, scales = 'free_x',
    labeller = labeller(gamma = label_both, p = label_both)) +
  theme_grey(base_family = 'mono') +
  theme(
    legend.position = "bottom",
    strip.text = element_text(family = "mono")) +
  labs(x = paste0("PC1(", expl_var[1], "%)")) +
  ggtitle("Density of PCA scores") +
  scale_x_continuous(breaks = scales::pretty_breaks(3)) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1")
}
pc_density_plot(pred_dta_with_pc, expl_var)
```


Figure \@ref(fig:pred-pca-hist-mthd-gamma-relpos) plots the score density from first principal component of minimum prediction error. Since higher prediction error results in high scores the plot shows that the PCR, PLS1 and PLS2 methods are influenced by two levels of position of relevant predictor components. When the position of relevant predictors are at positon `r opts$relpos[2]`, the eigenvalues corresponding to them becomes smaller making those designs difficult to model. However, the envelope methods have less influence of `relpos` in this regard. 

In addition, the plot also shows that the effect of `gamma`, the level of multicollinearity, has smaller effect in all cases. This indicates that the methods are somewhat robust to handle collinear predictors.

Further, the density curve for PCR, PLS1 and PLS2 for different levels of `eta`, the factor controlling the correlation between responses, are similar. However, this is not true for envelope models. The envelope methods have shown to have significant interaction between position of relevant components and `eta`. Here higher levels of `eta` is giving larger scores and clear separation between two level of `relpos`. This behavior is expected in the simultaneous envelope as the method has claimed to model relevant response (material) response space.

(ref:comp-hist) Density of first principal component of the matrix of number of principal components used to give minimum prediction error subdivided by methods, gamma and eta and grouped by relpos.

```{r comp-pca-hist-mthd-gamma-relpos, message=FALSE, warning=FALSE, fig.cap="(ref:comp-hist)", fig.pos="!htb"}
pc_density_plot(comp_dta_with_pc, comp_expl_var)
```


Figure \@ref(fig:comp-pca-hist-mthd-gamma-relpos) plots the score density from first principal component of matrix with number of components used to get minimum prediction error. Here, the higher scores suggest the use of more components to give minimum prediction error. The plot shows that the relevant predictor components at `r unique(opts$relpos)[2]` gives larger prediction error than that are at the position `r unique(opts$relpos)[1]`. The pattern is more distinct in large multicollinearity case and PCR and PLS methods.

The plot also shows noticeable results in the case of envelope methods. The methods have shown equally better performance at both levels of `relpos` and `gamma`.
