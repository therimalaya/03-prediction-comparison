Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{cook2015simultaneous,
  abstract =     {We introduce envelopes for simultaneously reducing the
                  predictors and the responses in multivariate linear
                  regression, so the regression then depends only on estimated
                  linear combinations of X and Y. We use a likelihood-based
                  objective function for estimating envelopes and then propose
                  algorithms for estimation of a simultaneous envelope as well
                  as for basic Grassmann manifold optimization. The asymptotic
                  properties of the resulting estimator are studied under
                  normality and extended to general distributions. We also
                  investigate likelihood ratio tests and information criteria
                  for determining the simultaneous envelope dimensions.
                  Simulation studies and real data examples show substantial
                  gain over the classical methods, like partial least squares,
                  canonical correlation analysis, and reduced-rank regression.
                  This article has supplementary material available online.},
  author =       {Cook, R. Dennis and Zhang, Xin},
  doi =          {10.1080/00401706.2013.872700},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Cook,
                  Zhang{\_}2015{\_}Simultaneous envelopes for multivariate
                  linear regression.pdf:pdf},
  issn =         15372723,
  journal =      {Technometrics},
  keywords =     {Canonical correlations,Envelope model,Grassmann
                  manifold,Partial least squares,Principal component
                  analysis,Reduced-rank regression,Sufficient dimension
                  reduction},
  number =       1,
  pages =        {11--25},
  publisher =    {Taylor {\&} Francis},
  title =        {{Simultaneous envelopes for multivariate linear regression}},
  volume =       57,
  year =         2015
}

@article{gangsei2016linear,
  author =       {{Gangsei L. E.} and {Alm{\o}y T.} and {S{\ae}b{\o} S.}},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Gangsei L. E.,
                  Alm{\o}y T., S{\ae}b{\o} S.{\_}2016{\_}Linear regression with
                  bivariate response variable containing missing data. An
                  empirical Bayes.pdf:pdf},
  journal =      {Communications in Statistics – Simulation and Computation},
  pmid =         26924880,
  title =        {{Linear regression with bivariate response variable containing
                  missing data. An empirical Bayes strategy to increase
                  prediction precision}},
  year =         2016
}

@book{Jolliffe2002,
  abstract =     {Principal component analysis is central to the study of
                  multivariate data. Although one of the earliest multivariate
                  techniques it continues to be the subject of much research,
                  ranging from new model- based approaches to algorithmic ideas
                  from neural networks. It is extremely versatile with
                  applications in many disciplines. The first edition of this
                  book was the first comprehensive text written solely on
                  principal component analysis. The second edition updates and
                  substantially expands the original version, and is once again
                  the definitive text on the subject. It includes core material,
                  current research and a wide range of applications. Its length
                  is nearly double that of the first edition. Researchers in
                  statistics, or in other fields that use principal component
                  analysis, will find that the book gives an authoritative yet
                  accessible account of the subject. It is also a valuable
                  resource for graduate courses in multivariate analysis. The
                  book requires some knowledge of matrix algebra. Ian Jolliffe
                  is Professor of Statistics at the University of Aberdeen. He
                  is author or co-author of over 60 research papers and three
                  other books. His research interests are broad, but aspects of
                  principal component analysis have fascinated him and kept him
                  busy for over 30 years.},
  archivePrefix ={arXiv},
  arxivId =      {arXiv:1011.1669v3},
  author =       {Jolliffe, I T},
  booktitle =    {Encyclopedia of Statistics in Behavioral Science},
  doi =          {10.2307/1270093},
  eprint =       {arXiv:1011.1669v3},
  file =
                  {:Users/therimalaya/Dropbox/Papers/References/Jolliffe{\_}2002{\_}Principal
                  Component Analysis.pdf:pdf},
  isbn =         0387954422,
  issn =         00401706,
  pmid =         21435900,
  title =        {{Principal Component Analysis, Second Edition}},
  year =         2002
}

@article{cook2010envelope,
  abstract =     {We propose a new parsimonious version of the classical
                  multivariate normal linear model, yielding a maximum
                  likelihood estimator (MLE) that is asymptoti- cally less
                  variable than the MLE based on the usual model. Our approach
                  is based on the construction of a link between the mean
                  function and the covariance ma- trix, using the minimal
                  reducing subspace of the latter that accommodates the former.
                  This leads to a multivariate regression model, which we call
                  the envelope model, where the number of parameters is
                  maximally reduced. The MLE from the envelope model can be
                  substantially less variable than the usual MLE, especially
                  when the mean function varies in directions that are
                  orthogonal to the directions of maximum variation for the
                  covariance matrix.},
  author =       {Cook, R Dennis and Li, Bing and Chiaromonte, Francesca},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Cook, Li,
                  Chiaromonte{\_}2010{\_}Envelope Models for Parsimonious and
                  Efficient Multivariate Linear Regression.pdf:pdf},
  isbn =         {1017-0405},
  issn =         10170405,
  journal =      {Statistica Sinica},
  keywords =     {discriminant analysis,functional data analysis,grassmann,ing
                  subspaces,invariant subspaces,manifolds,phrases,principal
                  components,reduc,reduced rank regression,sufficient dimension
                  reduction},
  number =       3,
  pages =        {927--1010},
  publisher =    {JSTOR},
  title =        {{Envelope Models for Parsimonious and Efficient Multivariate
                  Linear Regression}},
  volume =       20,
  year =         2010
}

@article{cook2015foundations,
  abstract =     {Envelopes were recently proposed by Cook, Li and Chiaromonte
                  as a method for reducing estimative and predictive variations
                  in multivariate linear regression. We extend their
                  formulation, proposing a general definition of an envelope and
                  a general framework for adapting envelope methods to any
                  estimation procedure. We apply the new envelope methods to
                  weighted least squares, generalized linear models and Cox
                  regression. Simulations and illustrative data analysis show
                  the potential for envelope methods to significantly improve
                  standard methods in linear discriminant analysis, logistic
                  regression and Poisson regression. Supplementary materials for
                  this article are available online.},
  author =       {Cook, R. Dennis and Zhang, Xin},
  doi =          {10.1080/01621459.2014.983235},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Cook,
                  Zhang{\_}2015{\_}Foundations for Envelope Models and
                  Methods.pdf:pdf},
  issn =         {1537274X},
  journal =      {Journal of the American Statistical Association},
  keywords =     {Generalized linear models,Grassmannians,Weighted least
                  squares},
  number =       510,
  pages =        {599--611},
  publisher =    {Taylor {\&} Francis},
  title =        {{Foundations for Envelope Models and Methods}},
  volume =       110,
  year =         2015
}

@article{Alm_y_1996,
  author =       {Alm{\o}y, Trygve},
  doi =          {10.1016/0167-9473(95)00006-2},
  file =
                  {:Users/therimalaya/Dropbox/Papers/References/Alm{\o}y{\_}1996{\_}A
                  simulation study on comparison of prediction methods when only
                  a few components are relevant.pdf:pdf},
  journal =      {Computational Statistics {\&} Data Analysis},
  month =        {jan},
  number =       1,
  pages =        {87--107},
  publisher =    {Elsevier {\{}BV{\}}},
  title =        {{A simulation study on comparison of prediction methods when
                  only a few components are relevant}},
  volume =       21,
  year =         1996
}

@article{gangsei2016theoretical,
  abstract =     {Methods for linear regression with multivariate response
                  variables are well described in statistical literature. In
                  this study we conduct a theoretical evaluation of the expected
                  squared prediction error in bivariate linear regression where
                  one of the response variables contains missing data. We make
                  the assumption of known covariance structure for the error
                  terms. On this basis, we evaluate three well-known estima-
                  tors: standard ordinary least squares, generalized least
                  squares, and a James–Stein inspired estimator. Theoretical
                  risk functions are worked out for all three estimators to
                  evaluate under which circumstances it is advantageous to take
                  the error covariance structure into account.},
  author =       {Gangsei, Lars Erik and Alm{\o}y, Trygve and S{\ae}b{\o},
                  Solve},
  doi =          {10.1080/03610926.2016.1222434},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Gangsei,
                  Alm{\o}y, S{\ae}b{\o}{\_}2016{\_}Theoretical evaluation of
                  prediction error in linear regression with a bivariate
                  response variable containi.pdf:pdf},
  issn =         {0361-0926},
  journal =      {Communications in Statistics - Theory and Methods},
  keywords =     {Bivariate linear regression,James–Stein
                  estimator,error,missing data,prediction,risk function.},
  number =       {just-accepted},
  pages =        {1--9},
  title =        {{Theoretical evaluation of prediction error in linear
                  regression with a bivariate response variable containing
                  missing data}},
  volume =       0926,
  year =         2016
}

@book{ripley2009stochastic,
  abstract =     {WILEY-INTERSCIENCE PAPERBACK SERIES The Wiley-Interscience
                  Paperback Series consists of selected books that have been
                  made more accessible to consumers in an effort to increase
                  global appeal and general circulation. With these new
                  unabridged softcover volumes, Wiley hopes to extend the lives
                  of these works by making them available to future generations
                  of statisticians, mathematicians, and scientists. ". . .this
                  is a very competently written and useful addition to the
                  statistical literature; a book every statistician should look
                  at and that many should study!" -Short Book Reviews,
                  International Statistical Institute ". . .reading this book
                  was an enjoyable learning experience. The suggestions and
                  recommendations on the methods make this book an excellent
                  reference for anyone interested in simulation. With its
                  compact structure and good coverage of material, it is an
                  excellent textbook for a simulation course." -Technometrics ".
                  . .this work is an excellent comprehensive guide to simulation
                  methods, written by a very competent author. It is especially
                  recommended for those users of simulation methods who want
                  more than a 'cook book'. -Mathematics Abstracts This book is a
                  comprehensive guide to simulation methods with explicit
                  recommendations of methods and algorithms. It covers both the
                  technical aspects of the subject, such as the generation of
                  random numbers, non-uniform random variates and stochastic
                  processes, and the use of simulation. Supported by the
                  relevant mathematical theory, the text contains a great deal
                  of unpublished research material, including coverage of the
                  analysis of shift-register generators, sensitivity analysis of
                  normal variate generators, analysis of simulation output, and
                  more.},
  annote =       {The Wiley-Interscience Paperback Series consists of selected
                  books that have been made more accessible to consumers in an
                  effort to increase global appeal and general circulation. With
                  these new unabridged softcover volumes, Wiley hopes to extend
                  the lives of these works by making them available to future
                  generations of statisticians, mathematicians, and scientists.
                  ". . .this is a very competently written and useful addition
                  to the statistical literature; a book every statistician
                  should look at and that many should study!" —Short Book
                  Reviews, International Statistical Institute ". . .reading
                  this book was an enjoyable learning experience. The
                  suggestions and recommendations on the methods [make] this
                  book an excellent reference for anyone interested in
                  simulation. With its compact structure and good coverage of
                  material, it [is] an excellent textbook for a simulation
                  course." —Technometrics ". . .this work is an excellent
                  comprehensive guide to simulation methods, written by a very
                  competent author. It is especially recommended for those users
                  of simulation methods who want more than a 'cook book'. "
                  —Mathematics Abstracts This book is a comprehensive guide to
                  simulation methods with explicit recommendations of methods
                  and algorithms. It covers both the technical aspects of the
                  subject, such as the generation of random numbers, non-uniform
                  random variates and stochastic processes, and the use of
                  simulation. Supported by the relevant mathematical theory, the
                  text contains a great deal of unpublished research material,
                  including coverage of the analysis of shift-register
                  generators, sensitivity analysis of normal variate generators,
                  analysis of simulation output, and more.},
  author =       {Ripley, B D},
  booktitle =    {Statistics},
  doi =          {10.1002/9780470316726},
  isbn =         0471818844,
  number =       {Mc},
  pages =        237,
  publisher =    {John Wiley {\&} Sons},
  title =        {{Stochastic Simulation}},
  volume =       2009,
  year =         1987
}

@article{helland2016algorithms,
  abstract =     {Partial least squares regression has been a very popular
                  method for prediction, first used by chemometricians, but also
                  now by statisticians in a number of applied fields. The method
                  can in a natural way be connected to a statistical model which
                  now has been ex- tended and further developed in terms of an
                  envelope model. Concentrating on the uni- variate case, the
                  model is here approached from several points of view. Several
                  estimators of the regression vector in this model are defined,
                  including the ordinary PLS estimator, the maximum likelihood
                  envelope estimator and a recently proposed Bayes PLS
                  estimator. The different estimators are compared with respect
                  to prediction error by systematic sim- ulations. The main
                  conclusion from the simulations seems to be that Bayes PLS
                  performs well compared to the other methods in virtually all
                  cases.},
  annote =       {From Duplicate 1 (Model and estimators for partial least
                  squares - Helland, Inge S.; S{\ae}b{\o}, Solve; Alm{\o}y,
                  Trygve; Rimal, Raju) From Duplicate 1 (Model and estimators
                  for partial least squares - Helland, Inge S; S{\ae}b{\o}, S;
                  Alm{\o}y, T; Rimal, R) unpublished},
  author =       {Helland, Inge Svein and Saeb{\o}, Solve and Alm{\o}y, Trygve
                  and Rimal, Raju and S{\ae}b{\o}, Solve and Alm{\o}y, Trygve
                  and Rimal, Raju},
  doi =          {10.1002/cem.3044},
  file =         {:Users/therimalaya/Dropbox/Papers/References//Helland et
                  al.{\_}2018{\_}Model and estimators for partial least squares
                  regression.pdf:pdf},
  issn =         08869383,
  journal =      {Journal of Chemometrics},
  keywords =     {Data Simulation,Expected Prediction error,Experimental
                  Design,Linear Model,Partial Least Squares
                  Regression,Prediction Ability,Principal Component
                  Regression,R-Package,bayes pls estimator,envelope
                  model,maximum likelihood envelope estima-,partial least
                  squares,partial least squares model,prediction,relevant
                  Components,simulation,tor},
  month =        {sep},
  number =       9,
  pages =        {e3044},
  publisher =    {Wiley Online Library},
  title =        {{Model and estimators for partial least squares regression}},
  volume =       32,
  year =         2018
}

@article{cook2016algorithms,
  abstract =     {Envelopes were recently proposed as methods for reducing
                  estimative variation in multivariate linear regression.
                  Estimation of an envelope usually involves optimization over
                  Grassmann manifolds. We propose a fast and widely applicable
                  one-dimensional (1D) algorithm for estimating an envelope in
                  general. We reveal an important structural property of
                  envelopes that facilitates our algorithm, and we prove both
                  Fisher consistency and n-consistency of the algorithm.},
  archivePrefix ={arXiv},
  arxivId =      {1403.4138},
  author =       {Cook, R. Dennis and Zhang, Xin},
  doi =          {10.1080/10618600.2015.1029577},
  eprint =       {1403.4138},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Cook,
                  Zhang{\_}2016{\_}Algorithms for Envelope Estimation.pdf:pdf},
  issn =         15372715,
  journal =      {Journal of Computational and Graphical Statistics},
  keywords =     {Envelopes,Grassmann manifold,Reducing subspaces},
  number =       1,
  pages =        {284--300},
  publisher =    {Taylor {\&} Francis},
  title =        {{Algorithms for Envelope Estimation}},
  volume =       25,
  year =         2016
}

@article{helland1990partial,
  abstract =     {The calibration method PLS1 is described in terms of the joint
                  covariance structure of the explanatory variables and the
                  predicted variable. In the population version it is possible
                  to give simple conditions for when the PLS algorithm stops
                  after a certain number of steps, and it turns out that the
                  resulting predictor is the same as the one given by principal
                  component regression. The concept of relevant components is
                  defined, and the relationship to factor analysis models is
                  discussed. Finally, the implications for the sample version of
                  PLS are considered, both for the case when it is used as a
                  prediction method, and for the case when scores and loadings
                  from PLS--in a similar way as the scores and loadings from
                  factor analysis--are used in the interpretation of data.},
  author =       {Helland, Inge S.},
  doi =          {10.2307/4616159},
  file =
                  {:Users/therimalaya/Dropbox/Papers/References/Helland{\_}1990{\_}Partial
                  least squares regression and statistical models.pdf:pdf},
  issn =         {0303-6898},
  journal =      {Scandinavian Journal of Statistics},
  keywords =     {exact definition,methodological motivation,multiple
                  test,simultaneous test},
  number =       2,
  pages =        {97--114},
  publisher =    {JSTOR},
  title =        {{Partial least squares regression and statistical models}},
  volume =       17,
  year =         1990
}

@article{Rimal2018,
  abstract =     {Data science is generating enormous amounts of data, and new
                  and advanced analytical methods are constantly being developed
                  to cope with the challenge of extracting information from such
                  “big-data”. Researchers often use simulated data to assess and
                  document the properties of these new methods, and in this
                  paper we present an extension to the R-package simrel, which
                  is a versatile and transparent tool for simulating linear
                  model data with an extensive range of adjustable properties.
                  The method is based on the concept of relevant components, and
                  is equivalent to the newly developed envelope model. It is a
                  multi-response extension of R-package simrel which is
                  available in R-package repository CRAN, and as simrel the new
                  approach is essentially based on random rotations of latent
                  relevant components to obtain a predictor matrix X, but in
                  addition we introduce random rotations of latent components
                  spanning a response space in order to obtain a multivariate
                  response matrix Y. The properties of the linear relation
                  between X and Y are defined by a small set of input parameters
                  which allow versatile and adjustable simulations. Sub-space
                  rotations also allow for generating data suitable for testing
                  variable selection methods in multi-response settings. The
                  method is implemented as an update to the R-package simrel.},
  author =       {Rimal, Raju and Alm{\o}y, Trygve and S{\ae}b{\o}, Solve},
  doi =          {10.1016/j.chemolab.2018.02.009},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Rimal, Alm{\o}y,
                  S{\ae}b{\o}{\_}2018{\_}A tool for simulating multi-response
                  linear model data.pdf:pdf},
  issn =         18733239,
  journal =      {Chemometrics and Intelligent Laboratory Systems},
  keywords =     {Data simulation,Linear model,Multivariate,Simrel package in r},
  month =        {may},
  pages =        {1--10},
  publisher =    {Elsevier},
  title =        {{A tool for simulating multi-response linear model data}},
  volume =       176,
  year =         2018
}

@article{Helland2000,
  abstract =     {We look at prediction in regression models under squared loss
                  for the random x case with many explanatory variables. Model
                  reduction is done by conditioning upon only a small number of
                  linear combinations of the original variables. The
                  corresponding reduced model will then essentially be the
                  population model for the chemometricians' partial least
                  squares algorithm. Estimation of the selection matrix under
                  this model is briefly discussed, and analoguous results for
                  the case with multivariate response are formulated. Finally,
                  it is shown that an assumption of multinormality may be
                  weakened to assuming elliptically symmetric distribution, and
                  that some of the results are valid without any distributional
                  assumption at all.},
  author =       {Helland, Inge S.},
  doi =          {10.1111/1467-9469.00174},
  file =
                  {:Users/therimalaya/Dropbox/Papers/References/Helland{\_}2000{\_}Model
                  Reduction for Prediction in Regression Models.pdf:pdf},
  issn =         {0303-6898},
  journal =      {Scandinavian Journal of Statistics},
  keywords =     {cation,classi,invariant space,mean squared prediction
                  error,model reduction,partial least squares
                  regression,prediction,random x,regression analysis},
  month =        {mar},
  number =       1,
  pages =        {1--20},
  publisher =    {Wiley-Blackwell},
  title =        {{Model Reduction for Prediction in Regression Models}},
  volume =       27,
  year =         2000
}

@book{Gamerman2006,
  abstract =     {A thorough, updated description of Markov chain Monte Carlo
                  methods Bridging the gap betweeen new research {\&}
                  application, Markov Chain Monte Carlo: Stochastic Simulation
                  for Bayesian Inference provides a concise, up-to-date, {\&}
                  integrated account of recent developments in Markov chain
                  Monte Carlo (MCMC) for performing Bayesian inference. This
                  volume, which was developed from a short course taught by the
                  author at a meeting of Brazilian statisticians {\&}
                  probabilists, retains the didactic character of the original
                  course text. The self-contained text units make MCMC
                  accessible to scientists in other disciplines as well as
                  statisticians. This book will appeal to everyone working with
                  MCMC techniques, especially research {\&} graduate
                  statisticians {\&} biostatisticians, {\&} scientists handling
                  data {\&} formulating models.},
  author =       {Gamerman, D and Lopes, H F},
  booktitle =    {Texts in statistical science},
  isbn =         9781584885870,
  pages =        245,
  publisher =    {Taylor {\&} Francis},
  title =        {{Markov Chain Monte Carlo: Stochastic Simulation for Bayesian
                  Inference, Second Edition}},
  volume =       1,
  year =         2006
}

@article{heiberger1978algorithm,
  author =       {Heiberger, Richard M},
  doi =          {10.2307/2346957},
  file =
                  {:Users/therimalaya/Dropbox/Papers/References/Heiberger{\_}1978{\_}Algorithm
                  AS 127 Generation of Random Orthogonal Matrices.pdf:pdf},
  issn =         00359254,
  journal =      {Applied Statistics},
  keywords =     {numerical,random unitary matrices},
  number =       2,
  pages =        199,
  publisher =    {JSTOR},
  title =        {{Algorithm AS 127: Generation of Random Orthogonal Matrices}},
  volume =       27,
  year =         1978
}

@article{Naes1985,
  abstract =     {In this paper we discuss the partial least squares (PLS)
                  prediction method. The method is compared to the predictor
                  based on principal component regression (PCR). Both
                  theoretical considerations and computations on artificial and
                  real data are presented.$\backslash$nIn this paper we discuss
                  the partial least squares (PLS) prediction method. The method
                  is compared to the predictor based on principal component
                  regression (PCR). Both theoretical considerations and
                  computations on artificial and real data are presented.},
  author =       {Naes, Tormod and Martens, Harald},
  doi =          {10.1080/03610918508812458},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Naes,
                  Martens{\_}1985{\_}Comparison of prediction methods for
                  multicollinear data.pdf:pdf},
  isbn =         0361091850,
  issn =         {0361-0918},
  journal =      {Communications in Statistics - Simulation and Computation},
  keywords =     {partial least squares regression,prediction,principal
                  component regression},
  month =        {jan},
  number =       3,
  pages =        {545--576},
  title =        {{Comparison of prediction methods for multicollinear data}},
  volume =       14,
  year =         1985
}

@article{saebo2015simrel,
  abstract =     {In the field of chemometrics and other areas of data analysis
                  the development of new methods for statistical inference and
                  prediction is the focus of many studies. The requirement to
                  document the properties of new methods is inevitable, and
                  often simulated data are used for this purpose. However, when
                  it comes to simulating data there are few standard approaches.
                  In this paper we propose a very transparent and versatile
                  method for simulating response and predictor data from a
                  multiple linear regression model which hopefully may serve as
                  a standard tool simulating linear model data. The approach
                  uses the principle of a relevant subspace for prediction,
                  which is known both from Partial Least Squares and envelope
                  models, and is essentially based on a re-parametrization of
                  the random x regression model. The approach also allows for
                  defining a subset of relevant observable predictor variables
                  spanning the relevant latent subspace, which is handy for
                  exploring methods for variable selection. The data properties
                  are defined by a small set of input-parameters defined by the
                  analyst. The versatile approach can be used to simulate a
                  great variety of data with varying properties in order to
                  compare statistical methods. The method has been implemented
                  in an R-package and its use is illustrated by examples.},
  author =       {S{\ae}b{\o}, Solve and Alm{\o}y, Trygve and Helland, Inge S.},
  doi =          {10.1016/j.chemolab.2015.05.012},
  file =         {:Users/therimalaya/Dropbox/Papers/References/S{\ae}b{\o},
                  Alm{\o}y, Helland{\_}2015{\_}Simrel - A versatile tool for
                  linear model data simulation based on the concept of a
                  relevant subspace and.pdf:pdf},
  issn =         18733239,
  journal =      {Chemometrics and Intelligent Laboratory Systems},
  keywords =     {Data simulation,Experimental design,Linear model,R-package},
  pages =        {128--135},
  publisher =    {Elsevier B.V.},
  title =        {{Simrel - A versatile tool for linear model data simulation
                  based on the concept of a relevant subspace and relevant
                  predictors}},
  volume =       146,
  year =         2015
}

@article{helland2012near,
  abstract =     {Abstract. The random x regression model is approached through
                  the group of rotations of the eigenvectors for the
                  x-covariance matrix together with scale transformations for
                  each of the corresponding regression coefficients. The partial
                  least squares model can be constructed from the orbits of this
                  group. A generalization of Pitman's Theorem says that the best
                  equivariant estimator under a group is given by the Bayes
                  estimator with the group's invariant measure as the prior. A
                  straightforward application of this theorem turns out to be
                  impossible since the relevant invariant prior leads to a
                  non-defined posterior. Nevertheless we can devise an
                  approximate scale group with a proper invariant prior leading
                  to a well-defined posterior distribution with a finite mean.
                  This Bayes estimator is explored using Markov chain Monte
                  Carlo technique. The estimator seems to require heavy
                  computations, but can be argued to have several nice
                  properties. It is also a valid estimator when
                  p{\textgreater}n.},
  author =       {Helland, Inge S. and Saeb{\o}, Solve and Tjelmeland, Ha Kon},
  doi =          {10.1111/j.1467-9469.2011.00770.x},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Helland,
                  Saeb{\o}, Tjelmeland{\_}2012{\_}Near Optimal Prediction from
                  Relevant Components.pdf:pdf},
  issn =         03036898,
  journal =      {Scandinavian Journal of Statistics},
  keywords =     {Bayesian estimation,Envelope
                  model,Equivariance,Group,Invariant measure,Markov chain Monte
                  Carlo,Partial least squares model,Prediction,Relevant
                  components},
  month =        {mar},
  number =       4,
  pages =        {695--713},
  publisher =    {Wiley Online Library},
  title =        {{Near Optimal Prediction from Relevant Components}},
  volume =       39,
  year =         2012
}

@article{Kiers2007,
  abstract =     {Regression tends to give very unstable and unreliable
                  regression weights when predictors are highly collinear.
                  Several methods have been pro- posed to counter this problem.
                  A subset of these do so by finding components that summarize
                  the information in thepredictorsandthe criterion variables.The
                  present paper compares six such methods (two of which are
                  almost completely new) to ordinary regression: Partial least
                  Squares (PLS), Principal Component regression (PCR), Principle
                  covariates regression, reduced rank regression, and two
                  variants of what is called power regression. The comparison is
                  mainly done by means of a series of simulation studies, in
                  which data are constructed in various ways, with different
                  degrees of collinearity and noise, and the methods are
                  compared in terms of their capability of recovering the
                  population regres- sion weights, as well as their prediction
                  quality for the complete population. It turns out that
                  recovery of regression weights in situations with collinearity
                  is often very poor by allmethods, unless the regression
                  weights lie in the subspace spanning the first few principal
                  components of the predictor variables. In those cases,
                  typically PLS andPCRgive the best recoveries of regression
                  weights.The picture is inconclusive, however, because,
                  especially in the study with more real life like simulated
                  data, PLS and PCR gave the poorest recoveries of regres- sion
                  weights in conditions with relatively low noise and
                  collinearity. It seems that PLS and PCR are particularly
                  indicated in cases with much collinearity, whereas in other
                  cases it is better to use ordinary regression.Asfar as
                  prediction is concerned: Prediction suffers far less from
                  collinearity than recovery of the regression weights.},
  author =       {Kiers, Henk A.L. and Smilde, Age K.},
  doi =          {10.1007/s10260-006-0025-5},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Kiers,
                  Smilde{\_}2007{\_}A comparison of various methods for
                  multivariate regression with highly collinear
                  variables.pdf:pdf},
  issn =         16182510,
  journal =      {Statistical Methods and Applications},
  keywords =     {Multicollinearity,Multivariate regression,PLS,Power
                  regression,Principal component regression,Principal covariate
                  regression},
  title =        {{A comparison of various methods for multivariate regression
                  with highly collinear variables}},
  year =         2007
}

@article{cook2016note,
  abstract =     {We propose a new algorithm for envelope estimation, along with
                  a new n-consistent method for computing starting values. The
                  new algorithm, which does not require optimization over a
                  Grassmannian, is shown by simulation to be much faster and
                  typically more accurate than the best existing algorithm
                  proposed by Cook and Zhang (2016).},
  author =       {Cook, R. Dennis and Forzani, Liliana and Su, Zhihua},
  doi =          {10.1016/j.jmva.2016.05.006},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Cook, Forzani,
                  Su{\_}2016{\_}A note on fast envelope estimation.pdf:pdf},
  issn =         10957243,
  journal =      {Journal of Multivariate Analysis},
  keywords =     {Envelopes,Grassmann manifold,Reducing subspaces},
  pages =        {42--54},
  publisher =    {Elsevier},
  title =        {{A note on fast envelope estimation}},
  volume =       150,
  year =         2016
}

@article{anderson1987generation,
  abstract =     {In order to generate a random orthogonal matrix distributed
                  according to Haar measure over the orthogonal group it is
                  natural to start with a matrix of normal random variables and
                  then factor it by the singular value decomposition. A more
                  efficient method is obtained by using Householder
                  transformations. We propose another alternative based on the
                  product of {\$}{\{}{\{}n(n - 1){\}}/2{\}}{\$} orthogonal
                  matrices, each of which represents an angle of rotation. Some
                  numerical comparisons of alternative methods are made.},
  author =       {Anderson, T. W. and Olkin, I. and Underhill, L. G.},
  doi =          {10.1137/0908055},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Anderson, Olkin,
                  Underhill{\_}1987{\_}Generation of Random Orthogonal
                  Matrices.pdf:pdf},
  issn =         {0196-5204},
  journal =      {SIAM Journal on Scientific and Statistical Computing},
  keywords =     {haar measure,monte carlo method,orthogonal matrix,random
                  matrix,random number generator,simulation},
  number =       2,
  pages =        {625--629},
  publisher =    {SIAM},
  title =        {{Generation of Random Orthogonal Matrices}},
  volume =       8,
  year =         1987
}

@book{cook2018envelope,
  abstract =     {Written by the leading expert in the field, this text reviews
                  the major new developments in envelope models and methods An
                  Introduction to Envelopes provides an overview of the theory
                  and methods of envelopes, a class of procedures for increasing
                  efficiency in multivariate analyses without altering
                  traditional objectives. The author offers a balance between
                  foundations and methodology by integrating illustrative
                  examples that show how envelopes can be used in practice. He
                  discusses how to use envelopes to target selected coefficients
                  and explores predictor envelopes and their connection with
                  partial least squares regression. The book reveals the
                  potential for envelope methodology to improve estimation of a
                  multivariate mean. The text also includes information on how
                  envelopes can be used in generalized linear models,
                  regressions with a matrix-valued response, and reviews work on
                  sparse and Bayesian response envelopes. In addition, the text
                  explores relationships between envelopes and other dimension
                  reduction methods, including canonical correlations,
                  reduced-rank regression, supervised singular value
                  decomposition, sufficient dimension reduction, principal
                  components, and principal fitted components. This important
                  resource: * Offers a text written by the leading expert in
                  this field * Describes groundbreaking work that puts the focus
                  on this burgeoning area of study * Covers the important new
                  developments in the field and highlights the most important
                  directions * Discusses the underlying mathematics and linear
                  algebra * Includes an online companion site with both R and
                  Matlab support Written for researchers and graduate students
                  in multivariate analysis and dimension reduction, as well as
                  practitioners interested in statistical methodology, An
                  Introduction to Envelopes offers the first book on the theory
                  and methods of envelopes.},
  author =       {Cook, R. Dennis},
  edition =      1,
  isbn =         9781119422952,
  keywords =     {Dimension
                  reduction,Envelopes,Estimation,MATHEMATICS,Multivariate
                  analysis.,Statistics},
  mendeley-tags ={Dimension
                  reduction,Envelopes,Estimation,MATHEMATICS,Multivariate
                  analysis.,Statistics},
  pages =        316,
  publisher =    {Hoboken, NJ : John Wiley {\&} Sons, 2018.},
  title =        {{An introduction to envelopes : dimension reduction for
                  efficient estimation in multivariate statistics}},
  year =         2018
}

@article{cook2013scaled,
  abstract =     {Efficient estimation of the regression coefficients is a
                  fundamental problem in multivariate linear regression. The
                  envelope model proposed by Cook et al. (2010) was shown to
                  have the potential to achieve substantial efficiency gains by
                  accounting for linear combinations of the response vector that
                  are essentially immaterial to coefficient estimation. This
                  requires in part that the distribution of those linear
                  combinations be invariant to changes in the nonstochastic
                  predictor vector. However, inference based on an envelope is
                  not invariant or equivariant under rescaling of the responses,
                  tending to limit application to responses that are measured in
                  the same or similar units. The efficiency gains promised by
                  envelopes often cannot be realized when the responses are
                  measured in different scales. To overcome this limitation and
                  broaden the scope of envelope methods, we propose a scaled
                  version of the envelope model, which preserves the potential
                  of the original envelope methods to increase efficiency and is
                  invariant to scale changes. Likelihood-based estimators are
                  derived and theoretical properties of the estimators are
                  studied in various circumstances. It is shown that estimating
                  appropriate scales for the responses can produce substantial
                  efficiency gains when the original envelope model offers none.
                  Simulations and an example are given to support the
                  theoretical claims.},
  author =       {Cook, R. Dennis and Su, Zhihua},
  doi =          {10.1093/biomet/ast026},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Cook,
                  Su{\_}2013{\_}Scaled envelopes Scale-invariant and efficient
                  estimation in multivariate linear regression.pdf:pdf},
  issn =         00063444,
  journal =      {Biometrika},
  keywords =     {Dimension reduction,Envelope model,Reducing
                  subspace,Similarity transformation},
  number =       4,
  pages =        {939--954},
  publisher =    {Oxford University Press},
  title =        {{Scaled envelopes: Scale-invariant and efficient estimation in
                  multivariate linear regression}},
  volume =       100,
  year =         2013
}

@article{aldrin2000multivariate,
  abstract =     {Multivariate regression is considered with emphasis on
                  prediction. Ordinary least squares tends to yield unstable
                  estimates and consequently uncertain predictions when there
                  are few observations in the training data compared to the
                  number of parameters to be estimated. Thus, there is a need
                  for methods that work well in such situations. This article
                  presents a new alternative prediction method based on
                  reduced-rank regression. The method of reduced-rank regression
                  uses a certain decomposition of the ordinary least squares
                  estimate of the matrix of regression coefficients, and shrinks
                  the last terms of this decomposition exactly to 0. I suggest a
                  new method with soft shrinkage of the terms in the
                  decomposition. Furthermore, the reduced-rank regression, as
                  well as the new softly shrunk reduced-rank regression, are
                  combined with principal components regression. The methods are
                  modified to handle missing observations in the response
                  variables. The various methods are compared through a
                  simulation study. Softly shrunk reduced-rank regression,
                  possibly combined with principal components, turns out to be
                  the best overall method, and the improvement over ordinary
                  least squares is particularly large in situations with few
                  observations.},
  author =       {Aldrin, Magne},
  doi =          {10.1080/00031305.2000.10474504},
  file =
                  {:Users/therimalaya/Dropbox/Papers/References/Aldrin{\_}2000{\_}Multivariate
                  prediction using softly shrunk reduced-rank
                  regression.pdf:pdf},
  issn =         15372731,
  journal =      {American Statistician},
  keywords =     {Missing observations,Multivariate regression,Principal
                  components regression,Ridge regression},
  number =       1,
  pages =        {29--34},
  publisher =    {Taylor {\&} Francis},
  title =        {{Multivariate prediction using softly shrunk reduced-rank
                  regression}},
  volume =       54,
  year =         2000
}

@book{golub2012matrix,
  abstract =     {Revised and updated, the third edition of Golub and Van Loan's
                  classic text in computer science provides essential
                  information about the mathematical background and algorithmic
                  skills required for the production of numerical software. This
                  new edition includes thoroughly revised chapters on matrix
                  multiplication problems and parallel matrix computations,
                  expanded treatment of CS decomposition, an updated overview of
                  floating point arithmetic, a more accurate rendition of the
                  modified Gram-Schmidt process, and new material devoted to
                  GMRES, QMR, and other methods designed to handle the sparse
                  unsymmetric linear system problem.},
  archivePrefix ={arXiv},
  arxivId =      {arXiv:1011.1669v3},
  author =       {Golub, Gene H and {Van Loan}, Charles F and Loan, C F V},
  booktitle =    {Book},
  doi =          {10.1063/1.3060478},
  eprint =       {arXiv:1011.1669v3},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Golub, Van Loan,
                  Loan{\_}2012{\_}Matrix computations.pdf:pdf},
  isbn =         0801854148,
  issn =         00319228,
  number =       8,
  pages =        48,
  pmid =         18273219,
  publisher =    {JHU Press},
  title =        {{Matrix computations}},
  volume =       3,
  year =         2012
}

@article{naes1993relevant,
  abstract =     {This paper treats definitions of "relevant components" in
                  regression. A component is defined as a linear combination of
                  the independent variables, and different requirements for such
                  components to be "relevant" for the prediction problem are
                  discussed. The relationship between the definitions is
                  investigated and their relationship to various regression
                  methods is discussed. A new regression method is proposed:
                  restricted principal component regression, which is based on
                  the concept of "relevant components". The theory is
                  illustrated by examples.},
  author =       {N{\ae}s, Tormod and Helland, Inge S},
  file =         {:Users/therimalaya/Dropbox/Papers/References/N{\ae}s,
                  Helland{\_}1993{\_}Relevant components in regression.pdf:pdf},
  isbn =         0900259450,
  journal =      {Scandinavian Journal of Statistics},
  keywords =     {collinearity,partial least squares
                  regression,prediction,principal component
                  regression,restricted principal component regression},
  number =       3,
  pages =        {239--250},
  publisher =    {JSTOR},
  title =        {{Relevant components in regression}},
  volume =       20,
  year =         1993
}

@article{indahl2005twist,
  abstract =     {A modification of the PLS1 algorithm is presented. Stepwise
                  optimization over a set of candidate loading weights obtained
                  by taking powers of the y--X correlations and X standard
                  deviations generalizes the classical PLS1 based on y--X
                  covariances and hence adds flexibility to the modelling. When
                  good linear predictions can be obtained, the suggested
                  approach often finds models with fewer and more interpretable
                  components. Good performance is demonstrated when compared
                  with the classical PLS1 on calibration benchmark data sets. An
                  important part of the comparisons is managed by a novel model
                  selection strategy. The selection is based on choosing the
                  simplest model among those with a cross-validation error
                  smaller than the pre-specified significance limit of a
                  X2-statistic.},
  author =       {Indahl, Ulf},
  doi =          {10.1002/cem.904},
  file =
                  {:Users/therimalaya/Dropbox/Papers/References/Indahl{\_}2005{\_}A
                  twist to partial least squares regression.pdf:pdf},
  isbn =         {0886-9383},
  issn =         08869383,
  journal =      {Journal of Chemometrics},
  keywords =     {Cross-validation,Model interpretation,Model
                  selection,PLS1,Powers of correlations and standard deviations},
  number =       1,
  pages =        {32--44},
  publisher =    {Wiley Online Library},
  title =        {{A twist to partial least squares regression}},
  volume =       19,
  year =         2005
}

@article{indahl2009canonical,
  abstract =     {We propose a new data compression method for estimating
                  optimal latent variables in multi-variate classification and
                  regression problems where more than one response variable is
                  available. The latent variables are found according to a
                  common innovative principle combining PLS methodology and
                  canonical correlation analysis (CCA). The suggested method is
                  able to extract predictive information for the latent
                  variablesmore effectively than ordinary PLS approaches. Only
                  simple modifications of existing PLS and PPLS algorithms are
                  required to adopt the proposed method},
  author =       {Indahl, Ulf G. and Liland, Kristian Hovde and N{\ae}s, Tormod},
  doi =          {10.1002/cem.1243},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Indahl, Liland,
                  N{\ae}s{\_}2009{\_}Canonical partial least squares-a unified
                  PLS approach to classification and regression
                  problems.pdf:pdf},
  issn =         08869383,
  journal =      {Journal of Chemometrics},
  keywords =     {Canonical correlation analysis,Discriminant analysis,Partial
                  least squares,Powered partial least squares,Regression with
                  several responses},
  number =       9,
  pages =        {495--504},
  publisher =    {Wiley Online Library},
  title =        {{Canonical partial least squares-a unified PLS approach to
                  classification and regression problems}},
  volume =       23,
  year =         2009
}

@article{DeJong1993,
  abstract =     {De Jong, S., 1993. SIMPLS: an alternative approach to partial
                  least squares regression. Chemometrics and Intelligent
                  Laboratory Systems, 18: 251-263. A novel algorithm for partial
                  least squares (PLS) regression, SIMPLS, is proposed which
                  calculates the PLS factors directly as linear combinations of
                  the original variables. The PLS factors are determined such as
                  to maximize a covariance criterion, while obeying certain
                  orthogonality and normalization restrictions. This approach
                  follows that of other traditional multivariate methods. The
                  construction of deflated data matrices as in the nonlinear
                  iterative partial least squares (NIPALS)-PLS algorithm is
                  avoided. For univariate y SIMPLS is equivalent to PLS1 and
                  closely related to existing bidiagonalization algorithms. This
                  follows from an analysis of PLS1 regression in terms of Krylov
                  sequences. For multivariate Y there is a slight difference
                  between the SIMPLS approach and NIPALS-PLS2. In practice the
                  SIMPLS algorithm appears to be fast and easy to interpret as
                  it does not involve a breakdown of the data sets.
                  {\textcopyright} 1993.},
  author =       {de Jong, Sijmen},
  doi =          {10.1016/0169-7439(93)85002-X},
  file =         {:Users/therimalaya/Dropbox/Papers/References/de
                  Jong{\_}1993{\_}SIMPLS An alternative approach to partial
                  least squares regression.pdf:pdf},
  isbn =         {0169-7439},
  issn =         01697439,
  journal =      {Chemometrics and Intelligent Laboratory Systems},
  month =        {mar},
  number =       3,
  pages =        {251--263},
  title =        {{SIMPLS: An alternative approach to partial least squares
                  regression}},
  volume =       18,
  year =         1993
}

@article{langsrud2005rotation,
  abstract =     {This paper describes a generalised framework for doing Monte
                  Carlo tests in multivariate linear regression.Therotation
                  methodologyassumes multivariate normality and is a true
                  generalisation of the classical multivariate tests - any
                  imaginable test statistic is allowed. The generalised test
                  statistics are dependent on the unknown covariance matrix.
                  Rotation testing handles this problem by conditioning on
                  sufficient statistics. Compared to permutation tests, we
                  replace permutations by proper random rotations. Permutation
                  tests avoid the multinormal assumption, but they are limited
                  to relatively simple models. On the other hand, a rotation
                  test can, in particular, be applied to any multivariate
                  generalisation of the univariate F-test. As an important
                  application, a detailed description of howeach single response
                  p-value can be non- conservatively adjusted for multiplicity
                  is given. This method is exact and non-conservative (unlike
                  Bonferroni), and it is a generalisation of the ordinary F-test
                  (except for the computation by simulations). Hence, this paper
                  offers an exact Monte Carlo solution to a classical problem of
                  multiple testing.},
  author =       {Langsrud, {\O}yvind},
  doi =          {10.1007/s11222-005-4789-5},
  file =
                  {:Users/therimalaya/Dropbox/Papers/References/Langsrud{\_}2005{\_}Rotation
                  tests.pdf:pdf},
  issn =         09603174,
  journal =      {Statistics and Computing},
  keywords =     {Adjusted p-value,Conditional inference,Microarray data
                  analysis,Multiple endpoints,Multiple testing,Random orthogonal
                  matrix,Spherical distribution},
  number =       1,
  pages =        {53--60},
  publisher =    {Springer},
  title =        {{Rotation tests}},
  volume =       15,
  year =         2005
}

@article{Helland1994b,
  abstract =     {We consider prediction in a multiple regression model where we
                  also look on the explanatory variables as random. If the
                  number of explanatory variables is large, then the common
                  least squares multiple regression solution may not be the best
                  one. We give a methodology for comparing certain alternative
                  prediction methods by asymptotic calculations and perform such
                  a comparisons for four specific methods. The results indicate
                  that none of these methods dominates the others, and that the
                  difference between the methods typically (but not always) is
                  small when the number of observations is large. In particular,
                  principal component regression does well when the eigenvalues
                  corresponding to components not correlated with the dependent
                  variables (i.e., the irrelevant eigenvalues) are extremely
                  small or extremely large. Partial least squares regression
                  does well for intermediate irrelevant eigenvalues. A maximum
                  likelihood-type method dominates the others asymptotically, at
                  least in the case of one relevant component.},
  author =       {Helland, Inge S. and Alm{\o}y, Trygve},
  doi =          {10.1080/01621459.1994.10476783},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Helland,
                  Alm{\o}y{\_}1994{\_}Comparison of prediction methods when only
                  a few components are relevant.pdf:pdf},
  issn =         {1537274X},
  journal =      {Journal of the American Statistical Association},
  keywords =     {Expected prediction error,Partial least squares
                  regression,Prediction ability,Principal component
                  regression,Relevant components},
  number =       426,
  pages =        {583--591},
  title =        {{Comparison of prediction methods when only a few components
                  are relevant}},
  volume =       89,
  year =         1994
}

@article{cook2013envelopes,
  abstract =     {We build connections between envelopes, a recently proposed
                  context for efficient estimation in multivariate statistics,
                  and multivariate partial least squares (PLS) regression. In
                  particular, we establish an envelope as the nucleus of both
                  univariate and multivariate PLS, which opens the door to
                  pursuing the same goals as PLS but using different envelope
                  estimators. It is argued that a likelihood-based envelope
                  estimator is less sensitive to the number of PLS components
                  that are selected and that it outperforms PLS in prediction
                  and estimation.},
  author =       {Cook, R. D. and Helland, I. S. and Su, Z.},
  doi =          {10.1111/rssb.12018},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Cook, Helland,
                  Su{\_}2013{\_}Envelopes and partial least squares
                  regression.pdf:pdf},
  issn =         13697412,
  journal =      {Journal of the Royal Statistical Society. Series B:
                  Statistical Methodology},
  keywords =     {Dimension reduction,Envelope models,Envelopes,Maximum
                  likelihood estimation,Partial least squares,SIMPLS algorithm},
  number =       5,
  pages =        {851--877},
  publisher =    {Wiley Online Library},
  title =        {{Envelopes and partial least squares regression}},
  volume =       75,
  year =         2013
}

@article{Cook2007a,
  abstract =     {Regressions in which the fixed number of predictors p exceeds
                  the number of independent observational units n occur in a
                  variety of scientific fields. Sufficient dimension reduction
                  provides a promising approach to such problems, by restricting
                  attention to d {\textless} n linear combinations of the
                  original p predictors. However, standard methods of sufficient
                  dimension reduction require inversion of the sample predictor
                  covariance matrix. We propose a method for estimating the
                  central subspace that eliminates the need for such inversion
                  and is applicable regardless of the (n, p) relationship.
                  Simulations show that our method compares favourably with
                  standard large sample techniques when the latter are
                  applicable. We illustrate our method with a genomics
                  application},
  author =       {Cook, R. Dennis and Li, Bing and Chiaromonte, Francesca},
  doi =          {10.1093/biomet/asm038},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Breslow,
                  Cain{\_}2016{\_}Biometrika Trust Logistic Regression for
                  Two-Stage Case-Control Data Published by Oxford University
                  Press on behalf.pdf:pdf},
  issn =         {0006-3444},
  journal =      {Biometrika},
  keywords =     {???-envelope,Central subspace,Singularity of sample
                  covariance},
  month =        {aug},
  number =       3,
  pages =        {569--584},
  title =        {{Dimension reduction in regression without matrix inversion}},
  volume =       94,
  year =         2007
}

@article{cook2015envlp,
  author =       {Cook, R. Dennis and Su, Zhihua and Yang, Yi},
  doi =          {10.18637/jss.v062.i08},
  file =         {:Users/therimalaya/Dropbox/Papers/References/Cook, Su,
                  Yang{\_}2015{\_}envlp A MATLAB Toolbox for Computing Envelope
                  Estimators in Multivariate Analysis.pdf:pdf},
  issn =         {1548-7660},
  journal =      {Journal of Statistical Software},
  keywords =     {dimension reduction,envelope models,grassmann,multivariate
                  linear regression},
  number =       8,
  pages =        {??--??},
  publisher =    {Foundation for Open Access Statistics},
  title =        {{envlp: A MATLAB Toolbox for Computing Envelope Estimators in
                  Multivariate Analysis}},
  volume =       62,
  year =         2015
}

@article{saebo2008lpls,
  abstract =     {A Partial Least Squares based approach is described which can
                  utilise relevant background information on dependencies
                  between predictor variables used for prediction or
                  classification. Within a wide range of research areas (e.g.
                  biomedicine, functional genomics, proteomics, chemometrics)
                  modern measurement technology has increased the possibility to
                  measure a very large number of variables on a given sample,
                  whereas the number of samples usually is limited. As is well
                  known, the large set of variables may cause many traditional
                  statistical methods to report a high number of false positives
                  due to collinearity and multiple testing issues. Further, most
                  existing methods for data modelling and variable selection do
                  not take advantage of possibly known dependencies between
                  variables. The modified LPLS-regression method proposed here
                  may take background knowledge on variables into account,
                  thereby increasing the accuracy of estimates and reducing the
                  number of false positives. The potential gain is better
                  variable selection and prediction. The LPLSR is an extension
                  of PLS-regression, where, in addition to response and
                  regressor matrices, an extra data matrix is constructed which
                  summarises the background information on the regressor
                  variables. We illustrate the potential of the LPLSR-approach
                  for this matter on both simulated and real data.
                  {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
  author =       {S{\ae}b{\o}, Solve and Alm{\o}y, Trygve and Flatberg, Arnar
                  and Aastveit, Are H. and Martens, Harald},
  doi =          {10.1016/j.chemolab.2007.10.006},
  file =         {:Users/therimalaya/Dropbox/Papers/References/S{\ae}b{\o} et
                  al.{\_}2008{\_}LPLS-regression a method for prediction and
                  classification under the influence of background information
                  on predictor.pdf:pdf},
  isbn =         {0169-7439},
  issn =         01697439,
  journal =      {Chemometrics and Intelligent Laboratory Systems},
  keywords =     {Breast cancer,L-shaped data matrix
                  structure,Microarray,Partial least squares regression,Pathway
                  information},
  number =       2,
  pages =        {121--132},
  publisher =    {Elsevier},
  title =        {{LPLS-regression: a method for prediction and classification
                  under the influence of background information on predictor
                  variables}},
  volume =       91,
  year =         2008
}
