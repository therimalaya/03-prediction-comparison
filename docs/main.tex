%review=doublespace preprint=single 5p=2 column
\documentclass[12pt,3p,authoryear]{elsarticle}

%% add packages %%
%% ------------ %%
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{caption}
\usepackage{subfig}
\usepackage{amssymb, amsmath}
\usepackage[inline]{enumitem}
\usepackage{float}
\usepackage{tabularx}
\usepackage[dvipsnames, table]{xcolor}
\usepackage{ifxetex, ifluatex}
\usepackage{fixltx2e}
\usepackage[unicode=true, colorlinks]{hyperref}
\usepackage{cleveref}
\usepackage{tabu}
\usepackage{mathpazo}
%% ------------ %%

%% Conditional Packages %%
%% -------------------- %%

\usepackage{easyReview}



% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}


\else % if luatex or xelatex
  \usepackage{fontspec}
  \ifxetex
    \usepackage{xltxtra,xunicode}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}



    \setmonofont{sourcecodepro}


\fi

% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}




\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}


\usepackage{longtable}




% Pandoc toggle for numbering sections (defaults to be off)
\setcounter{secnumdepth}{5}

%% Use Landscape Pages
\usepackage{lscape}

\usepackage{setspace}
\setstretch{1.5}

\usepackage{lmodern}

%% -------------------- %%

%% Create and Provide some customizations %%
%% -------------------------------------- %%
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
  
%% Custom macros
\newtheorem{mydef}{Definition}
\newcommand{\bs}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\diag}[1]{\mathrm{diag}\left(#1\right)}
\newcommand{\seq}[3][1]{\ensuremath{#2_{#1},\ldots,\,#2_{#3}}}
\newcommand{\note}[1]{\marginpar{\scriptsize\tt{\color{RoyalBlue}#1}}}
\newcommand{\edit}[1]{{\color{OrangeRed} #1}}

%% Declare Operators
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}

% set some lengths
\setlength{\parindent}{0pt}
% \setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines

%% Hyperref color setup
\AtBeginDocument{%
  %% Define Colors
  \newcommand\myshade{80}
  \colorlet{mylinkcolor}{violet!\myshade!black}
  \colorlet{mycitecolor}{YellowOrange!\myshade!black}
  \colorlet{myurlcolor}{Aquamarine!\myshade!black}

  \hypersetup{
    breaklinks = true,
    bookmarks  = true,
    pdfauthor  = {},
    pdftitle   = {Comparison of Multi-response Prediction Methods},
    linkcolor  = mylinkcolor,
    citecolor  = mycitecolor,
    urlcolor   = myurlcolor,
    colorlinks = true,
  }
}
% \urlstyle{same}  % don't use monospace font for urls
%% -------------------------------------- %%

%% Customizations %%
%% -------------- %%
 % turn line numbering on

%% -------------- %%

%% Configure Bibliography %%
%% ---------------------- %%
\bibliographystyle{elsarticle-harv}
\biboptions{numbers,sort&compress}

\makeatletter
\providecommand{\doi}[1]{%
  \begingroup
    \let\bibinfo\@secondoftwo
    \urlstyle{tt}%
    \href{http://dx.doi.org/#1}{%
      \discretionary{}{}{}%
      \nolinkurl{#1}%
    }%
  \endgroup
}
\makeatother

% 

%% Header Includes %%
%% --------------- %%
%% --------------- %%



\begin{document}
%% --- Front Matter Start --- %%
\begin{frontmatter}

  \title{Comparison of Multi-response Prediction Methods}
  
    \author[KBM]{Raju Rimal\corref{c1}}
   \ead{raju.rimal@nmbu.no} 
   \cortext[c1]{Corresponding Author}
    \author[KBM]{Trygve Almøy}
   \ead{trygve.almoy@nmbu.no} 
  
    \author[NMBU]{Solve Sæbø}
   \ead{solve.sabo@nmbu.no} 
  
      \address[KBM]{Faculty of Chemistry and Bioinformatics, Norwegian University of Life
Sciences, Ås, Norway}
    \address[NMBU]{Prorector, Norwegian University of Life Sciences, Ås, Norway}
  
  \begin{abstract}
  While Data science is battling to extract information from the enormous
  explosion of data, many estimators and algorithms are being developed
  for better prediction. Researchers and data scientists often introduce
  new methods and evaluate them based on various aspects of data. However,
  studies on the impact of/on model with multiple response model is
  limited. This study compares some newly-developed (envelope) and
  well-established (PLS, PCR) prediction methods based on simulated data
  specifically designed by varying properties such as multicollinearity,
  correlation between multiple responses and amount of information content
  in the predictor variables. This study aims to give some insight on
  these methods and help researcher to understand and use them for further
  study.
  \end{abstract}
   \begin{keyword} model-comparison,multi-response,simrel\end{keyword}

\end{frontmatter}

\section{Introduction}\label{introduction}

Prediction has been an essential components of modern data science,
weather it is statistical analysis or machine learning. Modern
technology has facilitated a massive explosion of data, however, such
data often contain irrelevant information consequently making prediction
difficult. Researchers are devising new methods and algorithms in order
to extract information to create robust predictive models. Mostly such
models contain predictor variables that are directly or indirectly
correlated with other predictor variables. In addition studies often
constitute of many response variables correlated with each other. These
interlinked relationships influence any study, whether it is predictive
modeling or inference.

Modern inter-disciplinary research fields such as chemometrics,
econometrics and bioinformatics are handling multi-response models
extensively. This paper attempts to compare some multivariate prediction
methods based on their prediction performance on linear model data with
specific properties. The properties includes correlation between
response variables, correlation between predictor variables, number of
predictor variables and the position of relevant predictor components.
These properties are discussed more in the
\protect\hyperlink{experimental-design}{Experimental Design} section.
Among others \citet{saebo2015simrel} and \citet{Alm_y_1996} have made a
similar comparison in the single response setting. In addition,
\citet{Rimal2018} has also made a basic comparison on some prediction
methods and their interaction with the data properties of a
multi-response model. The main aim of this paper is to present a
comprehensive comparison of contemporary prediction methods such as
simultaneous envelope estimation (Senv) \citep{cook2015simultaneous} and
envelope estimation in predictor space (Xenv) \citep{cook2010envelope}
with customary prediction methods such as Principal Component Regression
(PCR), Partial Least Squares Regression (PLS) using simulated dataset
with controlled properties. In the case of PLS, we have used PLS1 which
fits individual response separately and PLS2 which fits all the
responses together. An experimental design and the methods under
comparison are discussed further, followed by a brief discussion of the
strategy behind the data simulation.

\section{Simulation Model}\label{simulation-model}

Consider a model where the response vector \((\mathbf{y})\) with \(m\)
elements and predictor vector \((\mathbf{x})\) with \(p\) elements
follow a multivariate normal distribution as follows,

\begin{equation}
  \begin{bmatrix}
    \mathbf{y} \\ \mathbf{x}
  \end{bmatrix} \sim \mathcal{N}
  \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_y \\
      \boldsymbol{\mu}_x
    \end{bmatrix},
    \begin{bmatrix}
    \boldsymbol{\Sigma}_{yy} & \boldsymbol{\Sigma}_{yx} \\
    \boldsymbol{\Sigma}_{xy} & \boldsymbol{\Sigma}_{xx}
    \end{bmatrix}
  \right)
  \label{eq:model-1}
\end{equation}

where, \(\boldsymbol{\Sigma}_{yy}\) and \(\boldsymbol{\Sigma}_{xx}\) are
the variance-covariance matrices of \(\mathbf{y}\) and \(\mathbf{x}\),
respectively, \(\boldsymbol{\Sigma}_{xy}\) is the covariance between
\(\mathbf{x}\) and \(\mathbf{y}\) and \(\boldsymbol{\mu}_y\) and
\(\boldsymbol{\mu}_x\) are mean vectors of \(\mathbf{x}\) and
\(\mathbf{y}\), respectively. A linear model based on \eqref{eq:model-1}
is,

\begin{equation}
\mathbf{y} = \boldsymbol{\mu}_y + 
  \boldsymbol{\beta}^t(\mathbf{x} - \boldsymbol{\mu_x}) + 
  \boldsymbol{\epsilon}
\label{eq:reg-model-1}
\end{equation}

where, \(\underset{m\times p}{\boldsymbol{\beta}^t}\) is a matrix of
regression coefficients and \(\boldsymbol{\epsilon}\) is an error term
such that
\(\boldsymbol{\epsilon} \sim \mathcal{N}(0, \boldsymbol{\Sigma}_{y|x})\).
Here,
\(\boldsymbol{\beta}^t = \mathbf{\Sigma}_{yx}\mathbf{\Sigma}_{xx}^{-1}\)
and
\(\boldsymbol{\Sigma}_{y|x} = \boldsymbol{\Sigma}_{yy} - \boldsymbol{\Sigma}_{yx}\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}\)

In a model like \eqref{eq:reg-model-1}, we assume that the variation in
response \(\mathbf{y}\) is partly explained by the predictor
\(\mathbf{x}\). However, in many situations, only a subspace of the
predictor space is relevant for the variation in the response
\(\mathbf{y}\). This space can be referred to as the relevant space of
\(\mathbf{x}\) and the rest as irrelevant space. In the similar way, for
a certain model, we can assume that a subspace in the response space
exists which contains the information that the relevant space in
predictor can explain (Figure-\ref{fig:relevant-space}).
\citet{cook2010envelope} and \citet{cook2015simultaneous} have referred
to the relevant space as material space, and the irrelevant space as
immaterial space.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{main_files/figure-latex/relevant-space-1} 

}

\caption{Relevant space in a regression model}\label{fig:relevant-space}
\end{figure}

With an orthogonal transformation of \(\mathbf{y}\) and \(\mathbf{x}\)
to latent variables \(\mathbf{w}\) and \(\mathbf{z}\), respectively, by
\(\mathbf{w=Qy}\) and \(\mathbf{z = Rx}\), where \(\mathbf{Q}\) and
\(\mathbf{R}\) are orthogonal rotation matrices, an equivalent model to
\eqref{eq:model-1} in terms of the latent variables can be written as,

\begin{equation}
  \begin{bmatrix}
    \mathbf{w} \\ \mathbf{z}
  \end{bmatrix} \sim \mathcal{N}
  \left(
    \begin{bmatrix}
      \boldsymbol{\mu}_w \\
      \boldsymbol{\mu}_z
    \end{bmatrix},
    \begin{bmatrix}
    \boldsymbol{\Sigma}_{ww} & \boldsymbol{\Sigma}_{wz} \\
    \boldsymbol{\Sigma}_{zw} & \boldsymbol{\Sigma}_{zz}
    \end{bmatrix}
  \right)
  \label{eq:model-2}
\end{equation}

where, \(\boldsymbol{\Sigma}_{ww}\) and \(\boldsymbol{\Sigma}_{zz}\) are
the variance-covariance matrices of \(\mathbf{w}\) and \(\mathbf{z}\),
respectively. \(\boldsymbol{\Sigma}_{zw}\) is the covariance between
\(\mathbf{z}\) and \(\mathbf{w}\). \(\boldsymbol{\mu}_w\) and
\(\boldsymbol{\mu}_z\) are mean vector of \(\mathbf{z}\) and
\(\mathbf{w}\) respectively.

Here, the elements of \(\mathbf{w}\) and \(\mathbf{z}\) are the
principal components of responses and predictors, which will
respectively be referred as ``response components'' and ``predictor
components''. The column vectors of respective rotation matrices
\(\mathbf{Q}\) and \(\mathbf{R}\) are the eigenvectors corresponding to
these principal components. We can write a linear model based on
\eqref{eq:model-2} as,

\begin{equation}
\mathbf{w} = \boldsymbol{\mu}_w + \boldsymbol{\alpha}^t(\mathbf{z} - \boldsymbol{\mu_z}) + \boldsymbol{\tau}
\label{eq:reg-model-2}
\end{equation}

where, \(\underset{m\times p}{\boldsymbol{\alpha}^t}\) is a matrix of
regression coefficients and \(\boldsymbol{\tau}\) is an error term such
that
\(\boldsymbol{\tau} \sim \mathcal{N}(0, \boldsymbol{\Sigma}_{w|z})\).

Following the concept of relevant space, a subset of predictor
components can be imagined to span the predictor space. These components
can be regarded as relevant predictor components. \citet{Naes1985}
introduced the concept of relevant components which was explored further
by \citet{helland1990partial}, \citet{naes1993relevant},
\citet{Helland1994b} and \citet{Helland2000}. The corresponding
eigenvectors were referred to as relevant eigenvectors. A similar logic
is introduced by \citet{cook2010envelope} and later by
\citet{cook2013envelopes} as an envelope which is the space spanned by
the relevant eigenvectors \citep[pp.~101]{cook2018envelope}.

In addition, various simulation studies have been performed with the
model based on the concept of relevant subspace. A simulation study by
\citet{Alm_y_1996} has used a single response simulation model based on
reduced regression and has compared some contemporary multivariate
estimators. In the recent years \citet{helland2012near},
\citet{saebo2015simrel}, \citet{helland2016algorithms} and
\citet{Rimal2018} implemented similar simulation examples as we are
discussing in this study. This paper, however, presents an elaborate
comparision of the prediction using multi-response simulated linear
model data. The properties of the simulated data are varied through
different levels of simulation parameter based on an experimental
design. \citet{Rimal2018} has a detail discussion about the simulation
model that we have opted here. The following section presents the
estimators under comparison in more detail.

\section{Prediction Methods}\label{prediction-methods}

Partial least squares regression (PLS) and Principal component
regression (PCR) has been used in many disciplines such as chemometrics,
econometrics, bioinformatics and machine learning, where wide predictor
matrices, i.e. \(p\) (number or predictors) \textgreater{} \(n\) (number
of observation) is common. These methods are popular in multivariate
analysis, especially for exploratory studies and prediction. In recent
years, a concept of envelope introduced by \citet{Cook2007a} based on
reduction in regression model has been implemented for the development
of different estimators. In this study, we will follow estimation
methods based on their prediction performance on data simulated with
different controlled properties.

\begin{description}
\tightlist
\item[\emph{Principal Components Regression (PCR):}]
Principal components are the linear combinations of predictor variables
such that the transformation makes the new variables uncorrelated. In
addition the variation of the original dataset captured by the new
variables are sorted in descending order. In other words, each
successive components captures maximum variation left by the preceding
components in predictor variables \citep{Jolliffe2002}. Principal
components regression uses these principal components as a new
predictors to explain the variation in the response.
\item[\emph{Partial Least Squares (PLS):}]
Two variants of PLS: PLS1 and PLS2 will be used for comparison. The
first one considers individual response variables separately, i.e.~each
response is predicted with a single response model, while the latter
considers all response variables together. In PLS regression the
components are determined such as to maximize a covariance between
response and predictors \citep{DeJong1993}. R-package \texttt{pls}
\citep{pls2018} is used for both PCR and PLS methods.
\item[\emph{Envelopes:}]
The envelope, introduced by \citet{Cook2007a}, was first used to define
response envelope \citep{cook2010envelope} as a smallest subspace in the
response space such that the span of regression coefficients lies in
that space. Since a multivariate linear regression model contains
relevant (material) and irrelevant (immaterial) variation in both
response and predictor, the relevant part provides information, while
irrelevant part increases the estimative variation. The concept of
envelope uses the relevant part for estimation while excluding the
irrelevant part consequently increasing the efficiency of the model
\citep{cook2016algorithms}.

The concept was later extended to the predictor space, where the
predictor envelope was defined \citep{cook2013envelopes}. Further
\citet{cook2015simultaneous} uses envelopes for joint reduction of the
responses and predictors and argued to produce efficiency gains greater
than using individual envelops either of the response and predictors.
All the variants of envelope estimations are based on maximum likelihood
estimation. Here in this study we will also use predictor envelope
(Xenv) and simultaneous envelope (Senv) for the comparison. R-package
\texttt{Renvlp} \citep{env2018} is used for both Xenv and Senv methods.
\end{description}

\subsection{Modification in envelope
estimation}\label{modification-in-envelope-estimation}

Since envelope estimators (Xenv and Senv) are based on maximum
likelihood estimation (MLE), it fails to estimate in case of wide
matrices, i.e. \(p > n\). In order to incorporate these methods in our
comparison, we have used the principal components \((\mathbf{z})\) of
the predictor variables \((\mathbf{x})\) as predictors, using the
required number of components for capturing 97.5\% of the variation in
\(\mathbf{x}\). The new set of variables, \(\mathbf{z}\), were used for
envelope estimation. The regression coefficients
\((\hat{\boldsymbol{\alpha}})\) corresponding to these new variables
\(\mathbf{z}\) were transformed back to obtain coefficients for each
predictor variable as,

\[\hat{\boldsymbol{\beta}} = \mathbf{e}_k\hat{\boldsymbol{\alpha}_k}\]
where, \(\mathbf{e}_k\) is a matrix of eigenvectors with first \(k\)
number of components.

\hypertarget{experimental-design}{\section{Experimental
Design}\label{experimental-design}}

This study compares prediction methods based on their prediction
ability. Data with specific properties are simulated, some of which are
easier to predict than others. These data are simulated using the
R-package \texttt{simrel}, which is discussed in \citet{saebo2015simrel}
and \citet{Rimal2018}. Here we will use four different factors to vary
the property of the data: a) Number of predictors (\texttt{p}), b)
Multicollinearity in predictor variables (\texttt{gamma}), c)
Correlation in response variables (\texttt{eta}) and d) position of
predictor components relevant for the response (\texttt{relpos}). Using
two levels of \texttt{p}, \texttt{gamma} and \texttt{relpos} and four
levels of \texttt{eta}, 32 set of distinct properties are designed for
the simulation.

\begin{description}
\item[\textbf{Number of predictors:}]
In order to observe the performance of the methods on tall and wide
predictor matrices, 20 and 250 predictor variables are simulated.
Parameter \texttt{p} controls this properties in the \texttt{simrel}
function.
\item[\textbf{Multicollinearity in predictor variables:}]
Highly collinear predictors can be explained completely by few
components. The parameter \texttt{gamma} (\(\gamma\)) in \texttt{simrel}
controls decline in the eigenvalues of the predictor variables as
\eqref{eq:gamma}.

\begin{equation}
  \lambda_i = e^{-\gamma(i - 1)}, \gamma > 0 \text{ and } i = 1, 2, \ldots, p
  \label{eq:gamma}
\end{equation}
\end{description}

Here, \(\lambda_i, i = 1, 2, \ldots p\) are eigenvalues of the predictor
variables. Here we have used 0.2 and 0.9 as different levels of
\texttt{gamma}. The higher the value of gamma, the higher will be the
multicollinearity and vice versa.

\begin{description}
\item[\textbf{Correlation in response variables:}]
Correlation among response variables is a less explored area. Here we
have tried to explore that part with 4 levels of correlation in the
response variables. We have used the \texttt{eta} (\(\eta\)) parameter
of \texttt{simrel} for controlling the decline in eigenvalues
corresponding to the response variables as \eqref{eq:eta}.

\begin{equation}
  \kappa_j = e^{-\eta(j - 1)}, \eta > 0 \text{ and } j = 1, 2, \ldots, m
  \label{eq:eta}
\end{equation}
\end{description}

Here, \(\kappa_j, i = 1, 2, \ldots m\) are the eigenvalues of the
response variables and \texttt{m} is the number of response variables.
Here we have used 0, 0.4, 0.8 and 1.2 as different levels of
\texttt{eta}. The larger the value of eta, the larger will be the
correlation between response variables and vice versa.

\begin{description}
\tightlist
\item[\textbf{Position of predictor components relevant to the
response:}]
The principal components of the predictors are ordered. The first
principal component captures most of the variation in the predictors.
The second captures the most in the rest that is left by the first
principal components and so on. In highly collinear predictors, the
variation captured by the first few components is relatively high.
However, if those components are not relevant for the response,
prediction becomes difficult \citep{Helland1994b}. Here, two levels of
the positions of these relevant components are used: 1, 2, 3, 4 and 5,
6, 7, 8.
\end{description}

Further, a complete factorial design from the levels of the above given
parameters gave us 32 designs. Each design is associated with a dataset
having unique properties. Figure\textasciitilde{}\ref{fig:design-plot},
shows all the designs. For each design and prediction method, 50
datasets were simulated as replicates. In total, there were
\(5 \times 32 \times 50\), i.e.~8000 dataset simulated.

\begin{figure}
\includegraphics[width=1\linewidth]{main_files/figure-latex/design-plot-1} \caption{Experimental Design of simulation parameters. Each point represents an unique data property.}\label{fig:design-plot}
\end{figure}

\begin{description}
\tightlist
\item[\textbf{Common parameters:}]
Each dataset was simulated with \(n = 100\) number of observation and
\(m = 4\) response variables. Further, the coefficient of determination
corresponding to each response components in all the designs is set to
and 0.8. In addition, we have assumed that there is only one informative
response component. Hence, the informative response component is rotated
orthogonally together with three uninformative response components to
generate four response variables. This spread out the information in all
simulated response variables. For further details on the simulation tool
see \citep{Rimal2018}.
\end{description}

An example of simulation parameters for the first design is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{simrel}\NormalTok{(}
    \DataTypeTok{n       =} \DecValTok{100}\NormalTok{,                 ## Training samples}
    \DataTypeTok{p       =} \DecValTok{20}\NormalTok{,                  ## Predictors}
    \DataTypeTok{m       =} \DecValTok{4}\NormalTok{,                   ## Responses}
    \DataTypeTok{q       =} \DecValTok{20}\NormalTok{,                  ## Relevant predictors}
    \DataTypeTok{relpos  =} \KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)), ## Relevant predictor components index}
    \DataTypeTok{eta     =} \DecValTok{0}\NormalTok{,                   ## Decay factor of response eigenvalues}
    \DataTypeTok{gamma   =} \FloatTok{0.2}\NormalTok{,                 ## Decay factor of predictor eigenvalues}
    \DataTypeTok{R2      =} \FloatTok{0.8}\NormalTok{,                 ## Coefficient of determination}
    \DataTypeTok{ypos    =} \KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)),}
    \DataTypeTok{type    =} \StringTok{"multivariate"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=1\linewidth]{main_files/figure-latex/cov-plot-1-1} \caption{(left) Covariance structure of latent components. (right) Covariance structure of predictor and response}\label{fig:cov-plot-1}
\end{figure}

Figure \ref{fig:cov-plot-1} shows the covariance structure of the data
simulated with this design. The figure shows that the predictor
components at position 1, 2, 3 and 4 are relevant for the first response
component. After the rotation with orthogonal rotation matrix, all
predictors are somewhat relevant for all response variables, fulfilling
other desired properties like multicollinearity and coefficient of
determination. For the same design, Figure \ref{fig:est-cov-plot} (top
left) shows that the predictor components 1, 2, 3 and 4 are relevant for
the first response component. All other predictor components are
irrelevant and all other response components are uninformative. However,
due to orthogonal rotation of the informative response component
together with uninformative response components, all response variables
in the population have similar covariance with the relevant predictor
components (Figure \ref{fig:est-cov-plot}(top right)). The sample
covariances between the predictors components and predictor variables
with response variables are in Figure \ref{fig:est-cov-plot} (bottom
left) and (bottom right) respectively.













\begin{figure}
\includegraphics[width=1\linewidth]{main_files/figure-latex/est-cov-plot-1} \caption{Expected Scaled absolute covariance between predictor
components and response components (top left). Expected Scaled absolute
covariance between predictor components and response variables (top
right). Sample scaled absolute covariance between predictor components
and response variables (bottom left). Sample scaled absolute covariance
between predictor variables and response variables (bottom right). The
bar in the background are eigenvalues corresponding to each components
in population (top plots) and in sample (bottom plots). One can compare
the top-right plot (true covariance of the population) with bottom-left
(covariance in the simulated data) which shows a similar pattern for
different components.}\label{fig:est-cov-plot}
\end{figure}

A similar discussion can be made on all 32 designs where each of the
design holds the properties of the data they simulate. These data are
used by the prediction methods discussed in previous section. Each
prediction method is given independent datasets simulated in order to
give them equal opportunity to capture the dynamics in the data.

\section{Basis of comparison}\label{basis-of-comparison}

This study focuses mainly on the prediction performance of the methods
and emphasis specifically on the interaction between the properties of
the data controlled by the simulation parameters, and the prediction
methods. The prediction performance is measured on the following basis:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  The average prediction error that a method can give using arbitrary
  number of components and
\item
  The average number of components used by the method to give the
  minimum prediction error
\end{enumerate}

Let us define,

\begin{equation}
\mathcal{PE}_{ijkl} = \frac{1}{\sigma_{y_{ij}|x}^2}
  \mathsf{E}{\left[\left(\boldsymbol{\beta}_{ij} - 
  \boldsymbol{\hat{\beta}_{ijkl}}\right)^t
  \left(\boldsymbol{\Sigma}_{xx}\right)_i
  \left(\boldsymbol{\beta}_{ij} - \boldsymbol{\hat{\beta}_{ijkl}}\right)\right]} + 1
\label{eq:pred-error}
\end{equation}

as a prediction error of response \(j = 1, \ldots 4\) for a given design
\(i=1, 2, \ldots 32\) and method \(k=1(PCR), \ldots 5(Senv)\) using
\(l=0, \ldots 10\) number of components. Here,
\(\left(\boldsymbol{\Sigma}_{xx}\right)_i\) is the true covariance
matrix of the predictors, unique for a particular design \(i\) and
\(\sigma_{y_j\mid x}^2\) for response \(j = 1, \ldots m\) is the true
model error. Here prediction error is scaled by the true model error to
remove the effects of influencing residual variances. Since both the
expectation and the variance of \(\hat{\boldsymbol{\beta}}\) are
unknown, the prediction error are estimated using data from 50
replications as follows,

\begin{equation}
\widehat{\mathcal{PE}_{ijkl}} = \frac{1}{\sigma_{y_{ij}|x}^2}
  \sum_{r=0}^{50}{\left[\left(\boldsymbol{\beta}_{ij} - 
  \boldsymbol{\hat{\beta}_{ijklr}}\right)^t
  \left(\boldsymbol{\Sigma}_{xx}\right)_i
  \left(\boldsymbol{\beta}_{ij} - \boldsymbol{\hat{\beta}_{ijklr}}\right)\right]} + 1
\label{eq:estimated-pred-error}
\end{equation}

where, \(\widehat{\mathcal{PE}_{ijkl}}\) is the estimated prediction
error averaged over \(r=50\) replicates.

Following section focuses on the data for the estimation of these
prediction error that are used for the two models discussed above in a)
and b) of this section.

\section{Data Preparation}\label{data-preparation}

A dataset for estimating \eqref{eq:pred-error} is obtained from simulation
which contains five factors corresponding to simulation parameters,
prediction methods, number of components, replications and prediction
error for four responses. The prediction error is computed using 0 to 10
predictor components for each 50 replicates as,

\begin{equation*}
\left(\widehat{\mathcal{PE_\circ}}\right)_{ijklr} =
  \frac{1}{\sigma_{y_{ij}\mid x}^2}\left[
    \left(\boldsymbol{\beta}_{ij} - \hat{\boldsymbol{\beta}}_{ijklr}\right)^t
    \left(\boldsymbol{\Sigma}_{xx}\right)_{i}
    \left(\boldsymbol{\beta}_{ij} - \hat{\boldsymbol{\beta}}_{ijklr}\right)
  \right] + 1
\end{equation*}

.

Thus there are 32 (design) \(\times\) 5 (methods) \(\times\) 11 (number
of components) \(\times\) 50 (replications), i.e.~88000 observations
corresponding to the response variables from \texttt{Y1} to \texttt{Y4}.

Since we will focus our discussion on the average minimum prediction
error that a method can obtain and the average number of components they
use to get the minimum prediciton error in each replicates, the dataset
discussed above is summarized to construct following two smaller
datasets. Let us call them \emph{Error Dataset} and \emph{Component
Dataset}.

\begin{description}
\item[\emph{Error Dataset}:]
For each prediction method, design and response, an average prediction
error is computed over all replicates for each components. Next, a
component that gives the minimum of this average prediction error is
selected, i.e.,

\begin{equation}
  l_\circ = \operatorname*{argmin}_{l}\left[\frac{1}{50}\sum_{i=1}^{50}{\left(\mathcal{PE}_\circ\right)_{ijklr}}\right]
  \label{eq:min-pred}
  \end{equation}

Using the component \(l_\circ\), a dataset of
\(\left(\mathcal{PE}_\circ\right)_{ijkl_\circ r}\) is used as the
\emph{Error Dataset}. Let \(\mathbf{u}_{(8000 \times 4)} = (u_j)\) for
\(j = 1, \ldots 4\) be the outcome variables measuring the prediction
error corresponding to the response number \(j\) in the context of this
dataset.
\item[\emph{Component Dataset}:]
The component number that gives the minimum prediction error in each
replication is referred to as the \emph{Component Dataset}, i.e.,

\begin{equation}
  l_{\circ} = \operatorname*{argmin}_{l}\left[\mathcal{PE}_{ijklr}\right]
  \label{eq:min-comp}
  \end{equation}

Here \(l_\circ\) is the number of components that gives minimum
prediction error \(\left(\mathcal{PE}_\circ\right)_{ijklr}\) for design
\(i\), response \(j\), method \(k\) and replicate \(r\). Let
\(\mathbf{v}_{(8000 \times 4)} = (v_j)\) for \(j = 1, \ldots 4\) be the
outcome variables measuring the number of components used for minimum
prediction error corresponding the response \(j\) in the context of the
component dataset.
\end{description}

\section{Exploration}\label{exploration}

This section focuses on exploring the variation in the \emph{error
dataset} and the \emph{component dataset} for which we will use
Principal Component Analysis (PCA). Let \(\mathbf{t}_u\) and
\(\mathbf{t}_v\) be the principal component score sets corresponding to
PCA run on the \(\mathbf{u}\) and \(\mathbf{v}\) matrices respectively.
Figure-\ref{fig:pred-pca-hist-mthd-gamma-relpos} plots the scores
density corresponding to the first principal component of
\(\mathbf{u}\), i.e.~the first column of \(\mathbf{t}_u\).

Since higher prediction error here corresponds to high scores, the plot
shows that the PCR, PLS1 and PLS2 methods are influenced by the two
levels of position of relevant predictor components. When the relevant
predictors are at positions 5, 6, 7, 8, the eigenvalues corresponding to
them are relatively smaller. This also suggest that PCR, PLS1 and PLS2
depends highly on the position of the relevant components and the
variation of these components affect their prediction performance.
However, the envelope methods appears to be less influenced by
\texttt{relpos} in this regard.






\begin{figure}[!htb]
\includegraphics[width=1\linewidth]{main_files/figure-latex/pred-pca-hist-mthd-gamma-relpos-1} \caption{Scores density corresponding to first principal
component of \emph{error dataset} (\(\mathbf{u}\)) subdivided by
\texttt{methods}, \texttt{gamma} and \texttt{eta} and grouped by
\texttt{relpos}.}\label{fig:pred-pca-hist-mthd-gamma-relpos}
\end{figure}

In addition, the plot also shows that the effect of \texttt{gamma},
i.e., the level of multicollinearity, has smaller effect when the
relevant predictors are at positions 1, 2, 3, 4. This indicates that the
methods are somewhat robust to handle collinear predictors. Although,
when the relevant predictors are at positions 5, 6, 7, 8, high
multicollinearity results in small variance of these relevant components
and consequently gives poor prediction. This is in accordance with the
findings by \citet{Helland1994b}.

Further, the density curves for PCR, PLS1 and PLS2 are similar for
different levels of \texttt{eta}, i.e., the factor controlling the
correlation between responses. However, this is not true for the
envelope models. The envelope methods have shown to have distinct
interaction between position of relevant components and \texttt{eta}.
Here higher levels of \texttt{eta} has given larger scores and clear
separation between two level of \texttt{relpos}.

However in the case of high multicollinearity, envelope methods have
resulted in some large outliers. This suggests that in the case of
multicollinearity, the methods can give unexpected prediction.






\begin{figure}[!htb]
\includegraphics[width=1\linewidth]{main_files/figure-latex/comp-pca-hist-mthd-gamma-relpos-1} \caption{Score density corresponding to first principal component
of \emph{component dataset} (\(\mathbf{v}\)) subdivided by
\texttt{methods}, \texttt{gamma} and \texttt{eta} and grouped by
\texttt{relpos}.}\label{fig:comp-pca-hist-mthd-gamma-relpos}
\end{figure}

In the Figure \ref{fig:comp-pca-hist-mthd-gamma-relpos}, the higher
scores suggest the different methods have used a large number of
components to give minimum prediction error. The plot also shows that
the relevant predictor components at 5, 6, 7, 8 gives larger prediction
error than those which are at the position 1, 2, 3, 4. The pattern is
more distinct in large multicollinearity case and PCR and PLS methods.
Both the envelope methods have shown equally better performance at both
levels of \texttt{relpos} and \texttt{gamma}. However, for data with low
multicollinearity (\(\gamma = 0.2\)), the envelope methods have used
fewer number of components on average than in the high multicollinear
cases to achieve minimum prediction error.

\section{Statistical Analysis}\label{statistical-analysis}

In this section we will model the \emph{error data} and the
\emph{component data} as function of the simulation parameters in order
to better understand the connection between data properties and
prediction methods.

Let us consider a model with third order interaction of the simulation
parameters (\texttt{p}, \texttt{gamma}, \texttt{eta} and
\texttt{relpos}) and \texttt{Methods} as in \eqref{eq:error-model} and
\eqref{eq:component-model} using datasets \(\mathbf{u}\) and
\(\mathbf{v}\), respectively. Let us refer them as the \emph{error
model} and the \emph{component model}.

\begin{description}
\item[\textbf{Error Model:}]
\begin{equation}\mathbf{u}_{abcdef} = \boldsymbol{\mu}_u +
  (\texttt{p}_a + \texttt{gamma}_b + \texttt{eta}_c +
\texttt{relpos}_d + \texttt{Methods}_e)^3 +
  \left(\boldsymbol{\varepsilon}_u\right)_{abcdef}
  \label{eq:error-model}
  \end{equation}
\item[\textbf{Component Model:}]
\begin{equation}\mathbf{v}_{abcdef} = \boldsymbol{\mu}_v +
  (\texttt{p}_a + \texttt{gamma}_b + \texttt{eta}_c +
\texttt{relpos}_d + \texttt{Methods}_e)^3 +
  \left(\boldsymbol{\varepsilon}_v\right)_{abcdef}
  \label{eq:component-model}
  \end{equation}
\end{description}

where, \(\mathbf{u}_{abcdef}\) is a vector of prediction errors in the
\emph{error model} and \(\mathbf{v}_{abcdef}\) is a vector of number of
components used by a method to obtain minimum prediction error in the
\emph{component model}.

Although there are several test-statistic for MANOVA, for large samples
all are essentially equivalent \citep{johnson2018applied}. Here we will
use Pillai's trace statistic which is defined as,

\begin{equation}
\text{Pillai statistic} = \text{tr}\left[
\left(\mathbf{E} + \mathbf{H}\right)^{-1}\mathbf{H}
\right] = \sum_{i=1}^m{\frac{\nu_i}{1 + \nu_i}}
\label{eq:pillai}
\end{equation}

Here the matrix \(\mathbf{H}\) holds between-sum-of-squares and
sum-of-products for each of the predictors. The matrix \(\mathbf{E}\)
has a within sum of squares and sum of products for each of the
predictors. \(\nu_i\) represents the eigenvalues corresponding to
\(\mathbf{E}^{-1}\mathbf{H}\) \citep{rencher2003methods}.

For both the models \eqref{eq:error-model} and \eqref{eq:component-model},
Pillai's trace statistic is used for accessing the effect of each factor
and returns an F-value for the strength of their significance. Figure
\ref{fig:manova-plot} plots the Pillai's trace statistics as bars with
corresponding F-values as text labels for both models.





\begin{figure}
\includegraphics[width=1\linewidth]{main_files/figure-latex/manova-plot-1} \caption{Pillai Statistic and F-value for the MANOVA model. The
bar represents the Pillai Statistic and the text labels are F-value for
corresponding factor.}\label{fig:manova-plot}
\end{figure}

\begin{description}
\tightlist
\item[\emph{Error Model}:]
Figure \ref{fig:manova-plot} (left) shows the Pillai's trace statistic
for factors of the \emph{error model}. The main effects of
\texttt{Method} has largest influence on the model followed by
\texttt{relpos}, \texttt{eta} and \texttt{gamma}. A highly significant
two interaction of \texttt{Method} with \texttt{eta} followed by
\texttt{relpos} and \texttt{gamma} clearly shows that methods perform
differently for different levels of these data properties. Further, the
significant third order interaction between \texttt{Method},
\texttt{eta} and \texttt{gamma} suggests that a method performs
differently for a given level of multicollinearity and the correlation
between the responses. Since, only some methods consider modelling
predictor and response together, the prediction is affected by the level
of correlation between the response (\texttt{eta}) for a given method.
\item[\emph{Component Model}:]
Figure \ref{fig:manova-plot} (right) shows the Pillai's trace statistic
for factors of the \emph{component model}. As in the \emph{error model},
the main effects of the Method, \texttt{relpos}, \texttt{gamma} and
\texttt{eta} have significantly large effect on the number of components
that a method has used to obtain minimum prediction error. The two
factor interactions of \texttt{Method} with simulation parameters are
larger in this case. This shows that the Methods and these interactions
have larger effect on the use of number of component than the prediction
error itself. In addition, a similar significant high third order
interaction as in \emph{error model} is also observed in this model.
\end{description}

The following section will continue exploring the effects of different
levels of the factors in the case of these interactions.

\subsection{\texorpdfstring{Effect Analysis of \emph{Error
Model}}{Effect Analysis of Error Model}}\label{effect-analysis-of-error-model}

Figure \ref{fig:pred-eff-plots} (left) shows that, for the envelope
models, the differences in the prediction error is large. These
differences are intensified when position of relevant predictor at at 5,
6, 7, 8. The results also shows that the envelope methods is more
sensible to the levels of \texttt{eta} more than the rest of the
methods. In the case of PCR and PLS, the difference in the effect of
levels of \texttt{eta} is small.

In Figure \ref{fig:pred-eff-plots} (right), we can see that the
multicollinearity has affected all the methods, however, PCR, PLS1 and
PLS2 are more robust for the condition than the envelope methods. Rather
these methods have shown better performance when high multicollinearity
is present in the data. Envelope methods on the other hand are better at
handling the model when relevant positions are at 5, 6, 7, 8 in both
high and low multicollinearity cases.




\begin{figure}
\includegraphics[width=1\linewidth]{main_files/figure-latex/pred-eff-plots-1} \caption{Effect plot of some interactions of the multivariate
linear model of prediction error}\label{fig:pred-eff-plots}
\end{figure}

\subsection{\texorpdfstring{Effect Analysis of \emph{Component
Model}}{Effect Analysis of Component Model}}\label{effect-analysis-of-component-model}




\begin{figure}[!htb]
\includegraphics[width=1\linewidth]{main_files/figure-latex/comp-eff-plots-1} \caption{Effect plot of some interactions of the multivariate
linear model of number of components to get minimum prediction error}\label{fig:comp-eff-plots}
\end{figure}

Unlike for prediction errors, Figure \ref{fig:comp-eff-plots} (left)
shows that the number of components used by the methods to obtain minium
prediction error is less affected by the levels of \texttt{eta}.
However, simulteneous envelope has used slightly larger number of
components when there is no correlation between the response variables
than the cases with moderate correlation. This pattern is distinct
within the simultaneous envelope method. Envelope methods are able to
obtain minimum prediction error by using components ranging from 1 to 3
in both the cases of \texttt{relpos}. This value is much higher in the
case of PCR as its prediction is based only on the principal components
of predictors. The number of components used by this method ranges from
3 to 5 when relevant components are at positions 1, 2, 3, 4 and 5 to 8
when relevant components are at positions 5, 6, 7, 8.

We can also see that at \texttt{relpos} 1, 2, 3, 4 for PLS1 and PLS2
have used fewer components than simulteneous envelope. However, in the
case when relevant components are at position 5, 6, 7, 8, simulteneous
envelope manage to obtain smaller prediction error using fewer
components than that of the PLS models. This is the case when
eigenvalues of relevant predictors are small and responses are
relatively difficult to predict.

With regard to the interaction effect between \texttt{gamma},
\texttt{relpos} and \texttt{Method} (Figure \ref{fig:comp-eff-plots}
(right)), PCR, PLS1 and PLS2 methods have used a fairly large number of
components the cases of high multicollinearity and relevant predictors
at positions 5, 6, 7, 8. The number of components used by the envelope
methods in both cases of \texttt{relpos} is similar, although slightly
higher for the models with high multicollinearity than models with low
multicollinearity.

In the case of \texttt{PCR}, prediction relies heavily on the position
of relevant components (\texttt{relpos}). Here the method has used 3 to
5 number of components when the relevant components are at positions 1,
2, 3, 4; and 7 to 8 number of components when the relevant components
are at positions 5, 6, 7, 8. The number of components used by
\texttt{PCR} is expected as in Figure \ref{fig:comp-eff-plots}. This
reinforces the conclusion from the density plot (Figure
\ref{fig:comp-pca-hist-mthd-gamma-relpos}) in the previous section.

\section{Examples}\label{examples}

In addition to the analysis with the simulated data, following two
examples explores the prediction performance of the methods using real
datasets. Since both examples have wide predictor matrices, principal
components explaining 97.5\% of the variation in them are used for
envelope methods. The coefficients were transformed back after the
estimation.

\subsection{Raman spectra analysis of contents of polyunsaturated fatty
acids
(PUFA)}\label{raman-spectra-analysis-of-contents-of-polyunsaturated-fatty-acids-pufa}

This dataset contains 44 training samples and 25 test samples of fatty
acid information expressed as: a) percent of total sample weight and b)
percent of total fat content. The dataset is used from
\citet{naes2013multi} where more information can be found. The samples
were analysed using Raman spectroscopy from which 1096 variables are
obtained as predictors. Raman spectroscopy provide detailed chemical
information from minor components in food. The aim of this example is to
compare how well the prediction methods that we have considered are able
to predict the contents of PUFA using these Raman spectra.








\begin{figure}
\includegraphics[width=1\linewidth]{main_files/figure-latex/ex1-cumulative-eigevalues-1} \caption{(Left) Bar represents the eigenvalues
corresponding to Raman Spectra. The points and line are the covariance
between response and the principal components of Raman Spectra. All the
values are normalized to scale from 0 to 1. (Middle) Cumulative sum of
eigenvalues corresponding to predictors. (Right) Cumulative sum of
eigenvalues corresponding to responses.}\label{fig:ex1-cumulative-eigevalues}
\end{figure}

Figure \ref{fig:ex1-cumulative-eigevalues} (left) shows that the first
few predictor components are somewhat correlated with response
variables. In addition the most variation in predictors are explained by
less than five components (middle). Further, the response variables are
highly correlated suggesting that a single latent dimension explained
most of the variation (right). This resembles with the design 19 (Figure
\ref{fig:design-plot}) from our simulation.

\begin{figure}[!htb]
\includegraphics[width=1\linewidth]{main_files/figure-latex/ex1-prediction-error-1} \caption{Prediction Error of different prediction methods using different number of components.}\label{fig:ex1-prediction-error}
\end{figure}

Using components from 1 to 15, a regression model is fitted using each
of the methods. The fitted models are used to predict the test
observation and the root mean squared error of prediction (RMSEP) is
calculated. Figure \ref{fig:ex1-prediction-error} shows that PLS2 has
given minimum prediction error of 3.783 using 9 components in the case
of response \%Pufa while PLS1 has given minimum prediction error of
1.308 using 11 components in the case of response PUFA\%emul. However
the figure also shows that both envelope methods have reached to almost
minimum predicton error in few number of components. This pattern is
also visible in the simulation results (Figure
\ref{fig:pred-eff-plots}).

\subsection{Example-2: NIR spectra of biscuit
dough}\label{example-2-nir-spectra-of-biscuit-dough}

The dataset consists of 700 wavelengths of NIR spectra (1100--2498 nm in
steps of 2 nm) which we will use as predictor variables. There are four
response variables as the yield percentages of (a) fat, (b) sucrose, (c)
flour and (d) water. The measurements are taken from 40 training
observation of biscuit dough. A separate set with 32 samples which were
created and measured on different occasions are used as test
observations. The dataset is used from \citet{indahl2005twist} where
further information can be obtained.








\begin{figure}
\includegraphics[width=1\linewidth]{main_files/figure-latex/ex2-cumulative-eigevalues-1} \caption{(Left) Bar represents the eigenvalues
corresponding to NIR Spectra. The points and line are the covariance
between response and the principal components of NIR Spectra. All the
values are normalized to scale from 0 to 1. (Middle) Cumulative sum of
eigenvalues corresponding to predictors. (Right) Cumulative sum of
eigenvalues corresponding to responses.}\label{fig:ex2-cumulative-eigevalues}
\end{figure}

Figure \ref{fig:ex2-cumulative-eigevalues} (left) shows that the first
predictor component has largest variance and also has large covariance
with all response variables. The second component, however, has larger
variance (middle) than the succeeding components but has small
covaration with all the response which indicates that the component is
less relevant for any of the responses. In addition, two response
components have explained most of the variation in response variables
(right). This structure is also similar to Design 19.

\begin{figure}[!htb]
\includegraphics[width=1\linewidth]{main_files/figure-latex/ex2-prediction-error-1} \caption{Prediction Error of different prediction methods using different number of components.}\label{fig:ex2-prediction-error}
\end{figure}

Similar model is opted as in the first example and the fitted model from
each of 15 components are used for prediction. Figure
\ref{fig:ex2-prediction-error} shows the root mean squared error for
both test and train prediction. Here four different methods have minimum
test prediction error for four responses. As the structure of the data
is similar to the first example, the pattern in the prediction is also
similar for all methods.

The results from both of these examples also testify the simulation
results in Figure \ref{fig:pred-pca-hist-mthd-gamma-relpos} which has
shown large variation in prediction error in certain cases of envelope
method. However this also collaborate with Figure
\ref{fig:comp-pca-hist-mthd-gamma-relpos} which suggests that in most of
the designs, envelope methods have used less number of components to
achieve the minimum prediction error.

\section{Discussions and Conclusion}\label{discussions-and-conclusion}

On one hand, the envelope methods have shown better prediction
performance with fewer number of components. However, for data with high
multicollinearity, the methods have shown some cases of unstable
prediction. Here for the prediction error, we have used the components
that give the minimum average prediction error for all replicates. This
hints that for some replicates, the methods has used non-optimal number
of components and consequently resulting in high prediction error. The
two examples in the previous section collaborate with this point.

On the other hand, PLS1, PLS2 and PCR methods have smaller prediction
error in the model with high multicollinearity which suggest their
robustness in the particular case. However, they have shown poor
performance in modelling information at relevant components with small
variance. These methods have used larger number of components, in
general, than envelopes. Although resulting higher prediction error than
envelope methods in most situations, these methods are more stable
especially in the cases of high multicollinearity.

Further, we have fixed the coefficient of determination (\(R^2\))
constant throughtout all the designs. Initial simulations (not shown)
indicated that low \(R^2\) affect all methods in similar manner and the
MANOVA is hightly dominated by \(R^2\). Keeping the value of \(R^2\)
fixed has allowed us to analyze other factors properly.

Two clear comment can be made about the effect of correlation of
response on the prediction methods. Highly correlated response has shown
highest prediction error in general and the effect is distinct in
envelope methods. Since the envelope methods identifies the relevant
space as the span of relevant eigenvectors, the methods are able to
obtain the minimum average prediction error by using fewer number of
components for all levels of \texttt{eta}.

As of our knowledge, the effect of correlation in the response on PCR
and PLS methods are less explored. In this regards, it is interesting to
see that these methods have used large number of components and returned
large prediction error than envelope methods in the case of highly
correlated responses. In order to fully understand the effect of
\texttt{eta}, it is necessary to study the estimation performance of
these methods at different number of components.

In addition, since using principal components or actual variables as
predictors in envelope methods have shown similar results, we have used
principal components that have explained 97.5\% of the variation as
mentioned previously in the cases of envelope methods. As the envelope
methods are based on MLE and this can be an alternative way of using the
methods in data with wide predictors. The results from this study will
help researcher to understand these methods for different nature of
data. We encourage researcher to use newly developed methods such as
envlope based on the nature of data they are working on.

Since, this study have focused entirely on prediction performance,
further analysis of their estimative properties of these methods is
required. A study of estimation error and the behaviour of methods on
non-optimal number of components can give deeper understanding of these
methods.

A shiny application \citep{shiny} is availiable at
\url{http://therimalaya.shinyapps.io/Comparison} where all the results
related to this study can be visualized. In addition, a github
repository at
\url{https://github.com/therimalaya/03-prediction-comparison} can be
used to reproduce this study.

\section{Acknowledgment}\label{acknowledgment}

We are grateful to Inge Helland on his inputs on this paper throughout
the period. His guidance on the envelope models and review on this paper
helped us to shape this paper extensively. We would gratefully like to
thank Kristian Lillan, Ulf Indahl, Tormod Næs, Ingrid Måge and the team
for providing the data for analysis.

\hypertarget{refs}{}

\appendix



\renewcommand\refname{References}
\bibliography{ref-db.bib}


\end{document}
